{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fairlearn\n",
    "#! pip install lightgbm\n",
    "#! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, make_scorer, accuracy_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tqdm as notebook_tqdm\n",
    "from metrics import (\n",
    "    equality_opportunity_difference,\n",
    "    predictive_equality_difference,\n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "data = fetch_adult(as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == \">50K\") * 1\n",
    "A = X_raw[\"sex\"]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LGBMClassifier(n_jobs=-1),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directions = ['minimize', 'maximize']\n",
    "metric_scorer_decorated =  metrics(recall_score, predictive_equality_difference, sensitive_col = 'sex')\n",
    "for sim in [1]:\n",
    "    print(sim)\n",
    "    def objective(trial):\n",
    "\n",
    "        (X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "        X_raw, y, A, test_size=0.8, random_state=sim, stratify=y\n",
    "        )\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        A_train = A_train.reset_index(drop=True)\n",
    "        A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "        classifier_name = trial.suggest_categorical(\"classifier\", [\"logit\", \"RF\", 'GBM','LGBM'])\n",
    "\n",
    "        if classifier_name == \"logit\":        \n",
    "            params = {\n",
    "                \"penalty\" : trial.suggest_categorical('logit_penalty', ['l1','l2']),\n",
    "                \"c\" : trial.suggest_float('logit_c', 0.001, 10),\n",
    "                \"solver\" : 'saga'\n",
    "                }\n",
    "        \n",
    "        elif classifier_name ==\"RF\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "                'criterion': trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy']),\n",
    "                'max_depth': trial.suggest_int(\"rf_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_float(\"rf_min_samples_split\", 0.01, 1),\n",
    "                }\n",
    "\n",
    "        elif classifier_name ==\"LGBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"n_estimators\", 20, 10000),\n",
    "                'num_leaves': trial.suggest_int(\"num_leaves\", 10, 1000),\n",
    "                'max_depth': trial.suggest_int(\"max_depth\", 2, 20),\n",
    "                'min_child_samples': trial.suggest_int(\"min_child_samples\", 5, 300),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2),\n",
    "                'boosting_type': trial.suggest_categorical(\"boosting_type\", ['goss', 'gbdt'])\n",
    "                }\n",
    "        \n",
    "         elif classifier_name ==\"GBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"gbm_n_estimators\", 100, 1000),\n",
    "                'gbm_criterion': trial.suggest_categorical(\"gbm_criterion\", ['mse', 'friedman_mse']),\n",
    "                'max_depth': trial.suggest_int(\"gbm_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_int(\"gbm_min_child_samples\", 5, 300),\n",
    "                }\n",
    "        \n",
    "        else:\n",
    "            None\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        pipeline['classifier'].set_params(**params)\n",
    "\n",
    "        scores = cross_validate(\n",
    "                pipeline, \n",
    "                X_train,\n",
    "                y_train, \n",
    "                cv=5,\n",
    "                scoring = metric_scorer_decorated,\n",
    "                return_train_score=True)\n",
    "\n",
    "        fair_metric = scores['test_fairness'].mean()\n",
    "        model_metric = scores['test_model'].mean()\n",
    "\n",
    "        return fair_metric, model_metric\n",
    "    \n",
    "    \n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions = directions, \n",
    "        pruner = optuna.pruners.SuccessiveHalvingPruner(), \n",
    "        sampler = optuna.samplers.TPESampler() \n",
    "        )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    results.append(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "file_name = 'recall-fpr-lgbm-motpe-succesivehalving-100trials-4sim.pkl'\n",
    "#f1-eod-lgbm-succesivehalving-30trails.pkl\n",
    "with open(file_name, 'wb') as file:\n",
    "    dill.dump(results, file)\n",
    "    print(f'Object successfully saved to \"{file_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study, target_names=[\"FLOPS\", \"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
