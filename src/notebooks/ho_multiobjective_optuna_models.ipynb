{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fairlearn\n",
    "#! pip install lightgbm\n",
    "#! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, make_scorer, accuracy_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tqdm as notebook_tqdm\n",
    "from metrics import (\n",
    "    equality_opportunity_difference,\n",
    "    predictive_equality_difference,\n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "data = fetch_adult(as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == \">50K\") * 1\n",
    "A = X_raw[\"sex\"]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#pipeline = Pipeline(\n",
    "#    steps=[\n",
    "#        (\"preprocessor\", preprocessor),\n",
    "#        (\"classifier\", LGBMClassifier(n_jobs=-1)),\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 13:14:12,229]\u001b[0m A new study created in memory with name: no-name-b57d00b3-8a87-4581-ad75-12e6ff8c41f7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-15 13:14:27,285]\u001b[0m Trial 0 finished with values: [0.06621737438525606, 0.6328721243068138] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 531, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 2, 'gbm_min_child_samples': 291}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:14:39,053]\u001b[0m Trial 1 finished with values: [0.06790303397633626, 0.6277347682058603] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 409, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 2, 'gbm_min_child_samples': 114}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:15:02,546]\u001b[0m Trial 2 finished with values: [0.0758466666532088, 0.6294578963743847] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 5820, 'lgbm_num_leaves': 678, 'lgbm_max_depth': 9, 'lgbm_min_child_samples': 235, 'lgbm_learning_rate': 0.007754844265395161, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:15:04,675]\u001b[0m Trial 3 finished with values: [0.0844960733294109, 0.6046349676970662] and parameters: {'classifier': 'logit', 'logit_penalty': 'l1', 'logit_c': 1.5191354466396627}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:15:14,696]\u001b[0m Trial 4 finished with values: [0.06435778863237263, 0.6012152491809879] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 612, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 50}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:15:23,027]\u001b[0m Trial 5 finished with values: [0.06630403823779571, 0.6401590439063674] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 1984, 'lgbm_num_leaves': 426, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 51, 'lgbm_learning_rate': 0.007677442321430576, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:16:22,720]\u001b[0m Trial 6 finished with values: [0.08096743162067366, 0.641443840480243] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 9181, 'lgbm_num_leaves': 442, 'lgbm_max_depth': 17, 'lgbm_min_child_samples': 192, 'lgbm_learning_rate': 0.004392292978051796, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:16:26,722]\u001b[0m Trial 7 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 353, 'rf_criterion': 'gini', 'rf_max_depth': 1, 'rf_min_samples_split': 0.623851353872599}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:16:36,099]\u001b[0m Trial 8 finished with values: [0.08731841369148427, 0.6054887534544923] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 3136, 'lgbm_num_leaves': 152, 'lgbm_max_depth': 5, 'lgbm_min_child_samples': 198, 'lgbm_learning_rate': 0.006497145262370813, 'lgbm_boosting_type': 'goss'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:16:51,032]\u001b[0m Trial 9 finished with values: [0.06563389338762103, 0.6380131407968668] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 284, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 4, 'gbm_min_child_samples': 95}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:17:06,189]\u001b[0m Trial 10 finished with values: [0.02313819610432593, 0.33206500851040466] and parameters: {'classifier': 'RF', 'rf_n_estimators': 990, 'rf_criterion': 'entropy', 'rf_max_depth': 4, 'rf_min_samples_split': 0.014518966759026697}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:17:20,960]\u001b[0m Trial 11 finished with values: [0.02196726383654614, 0.3149389630117681] and parameters: {'classifier': 'RF', 'rf_n_estimators': 995, 'rf_criterion': 'entropy', 'rf_max_depth': 4, 'rf_min_samples_split': 0.04300556724534215}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:17:36,774]\u001b[0m Trial 12 finished with values: [0.024491108586849816, 0.3324859532568312] and parameters: {'classifier': 'RF', 'rf_n_estimators': 996, 'rf_criterion': 'entropy', 'rf_max_depth': 4, 'rf_min_samples_split': 0.011531480689057738}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:17:38,304]\u001b[0m Trial 13 finished with values: [0.08516425097185928, 0.6042067021724409] and parameters: {'classifier': 'logit', 'logit_penalty': 'l2', 'logit_c': 9.911301240926857}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:17:44,688]\u001b[0m Trial 14 finished with values: [0.06617477137921975, 0.6174701220739764] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 105, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 4, 'gbm_min_child_samples': 186}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:17:51,414]\u001b[0m Trial 15 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 649, 'rf_criterion': 'entropy', 'rf_max_depth': 2, 'rf_min_samples_split': 0.8120214066508741}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:18:43,119]\u001b[0m Trial 16 finished with values: [0.07335193649099761, 0.645719174948297] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 933, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 4, 'gbm_min_child_samples': 16}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:18:44,403]\u001b[0m Trial 17 finished with values: [0.08391373217062179, 0.6007851534618129] and parameters: {'classifier': 'logit', 'logit_penalty': 'l2', 'logit_c': 0.24477214662052926}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:18:52,203]\u001b[0m Trial 18 finished with values: [0.0002127659574468085, 0.05478504365014001] and parameters: {'classifier': 'RF', 'rf_n_estimators': 648, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.34600308389954093}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:18:58,595]\u001b[0m Trial 19 finished with values: [0.0657083925663032, 0.6149078497044236] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 110, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 4, 'gbm_min_child_samples': 155}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:11,751]\u001b[0m Trial 20 finished with values: [0.06375241600486678, 0.62902597046066] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 313, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 3, 'gbm_min_child_samples': 102}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:21,327]\u001b[0m Trial 21 finished with values: [0.0002127659574468085, 0.08686835410604148] and parameters: {'classifier': 'RF', 'rf_n_estimators': 710, 'rf_criterion': 'entropy', 'rf_max_depth': 4, 'rf_min_samples_split': 0.29389176809434736}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:32,247]\u001b[0m Trial 22 finished with values: [0.0002127659574468085, 0.07959790625743517] and parameters: {'classifier': 'RF', 'rf_n_estimators': 816, 'rf_criterion': 'entropy', 'rf_max_depth': 3, 'rf_min_samples_split': 0.25514638539578904}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:33,827]\u001b[0m Trial 23 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 107, 'rf_criterion': 'entropy', 'rf_max_depth': 4, 'rf_min_samples_split': 0.9729987701571281}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:44,242]\u001b[0m Trial 24 finished with values: [0.0, 0.008128809092406524] and parameters: {'classifier': 'RF', 'rf_n_estimators': 823, 'rf_criterion': 'entropy', 'rf_max_depth': 2, 'rf_min_samples_split': 0.21055160775808993}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:19:54,810]\u001b[0m Trial 25 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 818, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.45926189384705596}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:19:57,902]\u001b[0m Trial 26 finished with values: [0.08473962252175313, 0.6037784366478156] and parameters: {'classifier': 'logit', 'logit_penalty': 'l1', 'logit_c': 6.419895059403958}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:20:03,650]\u001b[0m Trial 27 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 441, 'rf_criterion': 'entropy', 'rf_max_depth': 1, 'rf_min_samples_split': 0.1307349781672809}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:20:17,307]\u001b[0m Trial 28 finished with values: [0.003710570784789586, 0.19684016910997637] and parameters: {'classifier': 'RF', 'rf_n_estimators': 895, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.16467204426772653}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:20:58,400]\u001b[0m Trial 29 finished with values: [0.07139910139276207, 0.6474249162685993] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 971, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 3, 'gbm_min_child_samples': 224}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:21:00,949]\u001b[0m Trial 30 finished with values: [0.08452798231011291, 0.6046349676970662] and parameters: {'classifier': 'logit', 'logit_penalty': 'l1', 'logit_c': 4.103563104242005}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:21:15,322]\u001b[0m Trial 31 finished with values: [0.027599863301939385, 0.3572969856695767] and parameters: {'classifier': 'RF', 'rf_n_estimators': 913, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.01743627332516674}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:21:45,630]\u001b[0m Trial 32 finished with values: [0.06931133861742968, 0.6457155145591976] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 720, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 3, 'gbm_min_child_samples': 84}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:21:52,059]\u001b[0m Trial 33 finished with values: [0.062372877490054615, 0.5776780321748202] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 315, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 235}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:22:27,398]\u001b[0m Trial 34 finished with values: [0.06940914771358796, 0.6478605025714235] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 767, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 3, 'gbm_min_child_samples': 10}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:22:37,430]\u001b[0m Trial 35 finished with values: [0.06645230014086148, 0.6200378850271784] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 307, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 2, 'gbm_min_child_samples': 140}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:22:42,047]\u001b[0m Trial 36 finished with values: [0.0, 0.0] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 78, 'lgbm_num_leaves': 953, 'lgbm_max_depth': 20, 'lgbm_min_child_samples': 17, 'lgbm_learning_rate': 0.00011450542181589383, 'lgbm_boosting_type': 'goss'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:23:16,641]\u001b[0m Trial 37 finished with values: [0.09270949475727014, 0.5841010999469244] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 9785, 'lgbm_num_leaves': 23, 'lgbm_max_depth': 13, 'lgbm_min_child_samples': 283, 'lgbm_learning_rate': 0.0036197393210812947, 'lgbm_boosting_type': 'goss'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:23:37,439]\u001b[0m Trial 38 finished with values: [0.06866470106531233, 0.6337295704533392] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 467, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 3, 'gbm_min_child_samples': 71}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:23:53,290]\u001b[0m Trial 39 finished with values: [0.02530834973068368, 0.3517359395303721] and parameters: {'classifier': 'RF', 'rf_n_estimators': 923, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.010472052178341629}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:39:56,469]\u001b[0m Trial 40 finished with values: [0.07577470651295125, 0.6234658394187302] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 6814, 'lgbm_num_leaves': 961, 'lgbm_max_depth': 2, 'lgbm_min_child_samples': 109, 'lgbm_learning_rate': 0.008526569292192138, 'lgbm_boosting_type': 'goss'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:40:09,606]\u001b[0m Trial 41 finished with values: [0.0002127659574468085, 0.11810611467999048] and parameters: {'classifier': 'RF', 'rf_n_estimators': 898, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.12373015949011185}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:40:24,529]\u001b[0m Trial 42 finished with values: [0.004786753212661625, 0.21138106480718905] and parameters: {'classifier': 'RF', 'rf_n_estimators': 890, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.15582568497630916}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:40:31,617]\u001b[0m Trial 43 finished with values: [0.0004362296445976465, 0.1146790753857135] and parameters: {'classifier': 'RF', 'rf_n_estimators': 453, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.13049012734144322}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:40:43,045]\u001b[0m Trial 44 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 898, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.467415477790462}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:40:55,193]\u001b[0m Trial 45 finished with values: [0.01380002459243595, 0.27000494152528415] and parameters: {'classifier': 'RF', 'rf_n_estimators': 749, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.11643365489169018}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:40:56,712]\u001b[0m Trial 46 finished with values: [0.08516425097185928, 0.6042067021724409] and parameters: {'classifier': 'logit', 'logit_penalty': 'l2', 'logit_c': 9.99842088586485}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:41:07,167]\u001b[0m Trial 47 finished with values: [0.0002127659574468085, 0.12238144914804445] and parameters: {'classifier': 'RF', 'rf_n_estimators': 719, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.12517264218390736}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:41:45,246]\u001b[0m Trial 48 finished with values: [0.08472670980099031, 0.6457283259210455] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 4018, 'lgbm_num_leaves': 631, 'lgbm_max_depth': 13, 'lgbm_min_child_samples': 113, 'lgbm_learning_rate': 0.009872778797395433, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:41:54,536]\u001b[0m Trial 49 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 725, 'rf_criterion': 'gini', 'rf_max_depth': 2, 'rf_min_samples_split': 0.3888406636128921}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:41:59,593]\u001b[0m Trial 50 finished with values: [0.0002127659574468085, 0.10098281447317851] and parameters: {'classifier': 'RF', 'rf_n_estimators': 278, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.2251406289922224}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:42:11,914]\u001b[0m Trial 51 finished with values: [0.010620780302445686, 0.255461300536247] and parameters: {'classifier': 'RF', 'rf_n_estimators': 746, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.11168774252593447}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:42:19,179]\u001b[0m Trial 52 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 580, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.5734191847859227}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:42:22,013]\u001b[0m Trial 53 finished with values: [0.08473962252175313, 0.6037784366478156] and parameters: {'classifier': 'logit', 'logit_penalty': 'l1', 'logit_c': 5.92784848002985}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:42:33,946]\u001b[0m Trial 54 finished with values: [0.000888718332380452, 0.14420285876388658] and parameters: {'classifier': 'RF', 'rf_n_estimators': 801, 'rf_criterion': 'gini', 'rf_max_depth': 3, 'rf_min_samples_split': 0.08422445496508259}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:01,327]\u001b[0m Trial 55 finished with values: [0.09282856145484154, 0.5806722304580977] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 7706, 'lgbm_num_leaves': 250, 'lgbm_max_depth': 17, 'lgbm_min_child_samples': 291, 'lgbm_learning_rate': 0.002311918245921088, 'lgbm_boosting_type': 'goss'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:03,242]\u001b[0m Trial 56 finished with values: [0.0, 0.0029914529914529917] and parameters: {'classifier': 'RF', 'rf_n_estimators': 106, 'rf_criterion': 'gini', 'rf_max_depth': 2, 'rf_min_samples_split': 0.2959078093858749}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:13,472]\u001b[0m Trial 57 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 830, 'rf_criterion': 'gini', 'rf_max_depth': 1, 'rf_min_samples_split': 0.07233003109990498}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:43:15,001]\u001b[0m Trial 58 finished with values: [0.08473962252175313, 0.6042067021724409] and parameters: {'classifier': 'logit', 'logit_penalty': 'l2', 'logit_c': 3.0718346257175653}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:20,397]\u001b[0m Trial 59 finished with values: [0.05991894212825997, 0.5686899467413385] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 269, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 299}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:24,975]\u001b[0m Trial 60 finished with values: [0.060650513805875314, 0.5567131536082285] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 218, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 295}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:37,866]\u001b[0m Trial 61 finished with values: [0.06588082298546812, 0.6260281117882831] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 410, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 2, 'gbm_min_child_samples': 249}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:43:42,443]\u001b[0m Trial 62 finished with values: [0.06063770943284359, 0.5515785427990996] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 210, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 182}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:02,076]\u001b[0m Trial 63 finished with values: [0.06725623381672732, 0.6384368308351178] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 631, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 2, 'gbm_min_child_samples': 261}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:10,189]\u001b[0m Trial 64 finished with values: [0.06297524861152098, 0.585381321034426] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 410, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 200}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:19,163]\u001b[0m Trial 65 finished with values: [0.06466373072984424, 0.6178974725013269] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 186, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 3, 'gbm_min_child_samples': 146}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:33,205]\u001b[0m Trial 66 finished with values: [0.0012857159961845535, 0.16432035725397612] and parameters: {'classifier': 'RF', 'rf_n_estimators': 887, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.2030405012919458}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:44,556]\u001b[0m Trial 67 finished with values: [0.06575793279808537, 0.6187540035505774] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 340, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 2, 'gbm_min_child_samples': 273}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:44:54,694]\u001b[0m Trial 68 finished with values: [0.06273916112096259, 0.5930855249913065] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 502, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 120}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:45:09,231]\u001b[0m Trial 69 finished with values: [0.001550740588475771, 0.16603250425520233] and parameters: {'classifier': 'RF', 'rf_n_estimators': 929, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.197617243489414}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:45:19,788]\u001b[0m Trial 70 finished with values: [0.0677494675754877, 0.6243214553707059] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 226, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 3, 'gbm_min_child_samples': 210}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:45:47,141]\u001b[0m Trial 71 finished with values: [0.07041444787934671, 0.6405763282636945] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 896, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 2, 'gbm_min_child_samples': 46}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:03,460]\u001b[0m Trial 72 finished with values: [0.025720173807758113, 0.3500311133073446] and parameters: {'classifier': 'RF', 'rf_n_estimators': 949, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.013406018527093607}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:14,906]\u001b[0m Trial 73 finished with values: [0.0649684209161529, 0.6012152491809879] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 608, 'gbm_criterion': 'squared_error', 'gbm_max_depth': 1, 'gbm_min_child_samples': 161}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:28,154]\u001b[0m Trial 74 finished with values: [0.0017651614568565475, 0.1681674261974048] and parameters: {'classifier': 'RF', 'rf_n_estimators': 877, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.2036204969153782}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:44,662]\u001b[0m Trial 75 finished with values: [0.06586739957084639, 0.6358708980764656] and parameters: {'classifier': 'GBM', 'gbm_n_estimators': 370, 'gbm_criterion': 'friedman_mse', 'gbm_max_depth': 3, 'gbm_min_child_samples': 172}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:47,974]\u001b[0m Trial 76 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 260, 'rf_criterion': 'gini', 'rf_max_depth': 4, 'rf_min_samples_split': 0.6324069514054688}. \u001b[0m\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/avasquez/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-15 13:46:50,839]\u001b[0m Trial 77 finished with values: [0.08473962252175313, 0.6042067021724409] and parameters: {'classifier': 'logit', 'logit_penalty': 'l1', 'logit_c': 7.515745485564496}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:54,417]\u001b[0m Trial 78 finished with values: [0.03720606451115009, 0.4604284485440802] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 218, 'lgbm_num_leaves': 680, 'lgbm_max_depth': 10, 'lgbm_min_child_samples': 93, 'lgbm_learning_rate': 0.005671772822394241, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:46:57,568]\u001b[0m Trial 79 finished with values: [0.03416345159559705, 0.4475969545562693] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 215, 'lgbm_num_leaves': 772, 'lgbm_max_depth': 9, 'lgbm_min_child_samples': 93, 'lgbm_learning_rate': 0.005597102658921813, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:47:05,399]\u001b[0m Trial 80 finished with values: [0.06823585402217003, 0.5943694064679077] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 694, 'lgbm_num_leaves': 767, 'lgbm_max_depth': 9, 'lgbm_min_child_samples': 86, 'lgbm_learning_rate': 0.005477672873832408, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:47:21,975]\u001b[0m Trial 81 finished with values: [0.07620436707353948, 0.6405799886527939] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 2206, 'lgbm_num_leaves': 792, 'lgbm_max_depth': 10, 'lgbm_min_child_samples': 149, 'lgbm_learning_rate': 0.005895768727151364, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:47:29,955]\u001b[0m Trial 82 finished with values: [0.06109684675767071, 0.5982192207031607] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 1247, 'lgbm_num_leaves': 557, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 58, 'lgbm_learning_rate': 0.003792164188114997, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:47:38,084]\u001b[0m Trial 83 finished with values: [0.06107347096941904, 0.5947958417979832] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 1269, 'lgbm_num_leaves': 580, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 55, 'lgbm_learning_rate': 0.003618240358094378, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:47:39,076]\u001b[0m Trial 84 finished with values: [0.0, 0.0] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 44, 'lgbm_num_leaves': 498, 'lgbm_max_depth': 4, 'lgbm_min_child_samples': 8, 'lgbm_learning_rate': 0.0048666213272698594, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:48:10,178]\u001b[0m Trial 85 finished with values: [0.06561596236555109, 0.6422930507512948] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 4040, 'lgbm_num_leaves': 780, 'lgbm_max_depth': 8, 'lgbm_min_child_samples': 66, 'lgbm_learning_rate': 0.002572450446677101, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:48:43,760]\u001b[0m Trial 86 finished with values: [0.06673039376846747, 0.6495662438917258] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 4242, 'lgbm_num_leaves': 810, 'lgbm_max_depth': 8, 'lgbm_min_child_samples': 66, 'lgbm_learning_rate': 0.002611464636481741, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:49:01,722]\u001b[0m Trial 87 finished with values: [0.07377556191544948, 0.6465738758029979] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 1786, 'lgbm_num_leaves': 677, 'lgbm_max_depth': 12, 'lgbm_min_child_samples': 102, 'lgbm_learning_rate': 0.006079421062165135, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:49:22,703]\u001b[0m Trial 88 finished with values: [0.06317963732969772, 0.6452881641318472] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 3126, 'lgbm_num_leaves': 331, 'lgbm_max_depth': 7, 'lgbm_min_child_samples': 40, 'lgbm_learning_rate': 0.004253521029558971, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:49:49,417]\u001b[0m Trial 89 finished with values: [0.07867980625532185, 0.6405873094309925] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 2658, 'lgbm_num_leaves': 302, 'lgbm_max_depth': 15, 'lgbm_min_child_samples': 140, 'lgbm_learning_rate': 0.005120984071633502, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:11,588]\u001b[0m Trial 90 finished with values: [0.0827148106648754, 0.6388779077215908] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 5132, 'lgbm_num_leaves': 342, 'lgbm_max_depth': 11, 'lgbm_min_child_samples': 30, 'lgbm_learning_rate': 0.00659980193761529, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:14,285]\u001b[0m Trial 91 finished with values: [0.04871944714667973, 0.5220510990318271] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 771, 'lgbm_num_leaves': 191, 'lgbm_max_depth': 3, 'lgbm_min_child_samples': 87, 'lgbm_learning_rate': 0.0045558828196916755, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:17,240]\u001b[0m Trial 92 finished with values: [0.05282286007657161, 0.5361737952744376] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 954, 'lgbm_num_leaves': 188, 'lgbm_max_depth': 3, 'lgbm_min_child_samples': 84, 'lgbm_learning_rate': 0.00423034754249043, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:19,531]\u001b[0m Trial 93 finished with values: [0.051473451896248565, 0.5267574443163309] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 1037, 'lgbm_num_leaves': 119, 'lgbm_max_depth': 2, 'lgbm_min_child_samples': 128, 'lgbm_learning_rate': 0.005298706034876521, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:21,501]\u001b[0m Trial 94 finished with values: [0.008505183642114004, 0.32735866322590096] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 186, 'lgbm_num_leaves': 59, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 44, 'lgbm_learning_rate': 0.004270095492264655, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:22,930]\u001b[0m Trial 95 finished with values: [0.0011617776112059425, 0.22765149435384982] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 126, 'lgbm_num_leaves': 18, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 37, 'lgbm_learning_rate': 0.004089975138551773, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:24,243]\u001b[0m Trial 96 finished with values: [0.00093553326731454, 0.21096286535258696] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 109, 'lgbm_num_leaves': 16, 'lgbm_max_depth': 7, 'lgbm_min_child_samples': 35, 'lgbm_learning_rate': 0.004251271609387579, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:28,317]\u001b[0m Trial 97 finished with values: [0.05254093817310802, 0.5468721975145958] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 820, 'lgbm_num_leaves': 114, 'lgbm_max_depth': 4, 'lgbm_min_child_samples': 39, 'lgbm_learning_rate': 0.004076441340728293, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:32,598]\u001b[0m Trial 98 finished with values: [0.05304562410671286, 0.5468749428064203] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 770, 'lgbm_num_leaves': 87, 'lgbm_max_depth': 5, 'lgbm_min_child_samples': 39, 'lgbm_learning_rate': 0.003757339757209905, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n",
      "\u001b[32m[I 2023-05-15 13:51:34,005]\u001b[0m Trial 99 finished with values: [0.0006953588045848129, 0.189994326396896] and parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 95, 'lgbm_num_leaves': 72, 'lgbm_max_depth': 6, 'lgbm_min_child_samples': 43, 'lgbm_learning_rate': 0.004705693388054579, 'lgbm_boosting_type': 'gbdt'}. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directions = ['minimize', 'maximize']\n",
    "metric_scorer_decorated =  metrics(recall_score, predictive_equality_difference, sensitive_col = 'sex')\n",
    "for sim in [3]:\n",
    "    print(sim)\n",
    "    def objective(trial):\n",
    "\n",
    "        (X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "        X_raw, y, A, test_size=0.8, random_state=sim, stratify=y\n",
    "        )\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        A_train = A_train.reset_index(drop=True)\n",
    "        A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "        classifier_name = trial.suggest_categorical(\"classifier\", [\"logit\", \"RF\", 'GBM','LGBM'])\n",
    "\n",
    "        if classifier_name == \"logit\":        \n",
    "            params = {\n",
    "                \"penalty\" : trial.suggest_categorical('logit_penalty', ['l1','l2']),\n",
    "                \"C\" : trial.suggest_float('logit_c', 0.001, 10),\n",
    "                \"solver\" : 'saga'\n",
    "                }\n",
    "            classifier = LogisticRegression(**params)\n",
    "        \n",
    "        elif classifier_name ==\"RF\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "                'criterion': trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy']),\n",
    "                'max_depth': trial.suggest_int(\"rf_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_float(\"rf_min_samples_split\", 0.01, 1),\n",
    "                }\n",
    "            classifier = RandomForestClassifier(**params)\n",
    "\n",
    "        elif classifier_name ==\"LGBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"lgbm_n_estimators\", 20, 10000),\n",
    "                'num_leaves': trial.suggest_int(\"lgbm_num_leaves\", 10, 1000),\n",
    "                'max_depth': trial.suggest_int(\"lgbm_max_depth\", 2, 20),\n",
    "                'min_child_samples': trial.suggest_int(\"lgbm_min_child_samples\", 5, 300),\n",
    "                'learning_rate': trial.suggest_float('lgbm_learning_rate', 1e-5, 1e-2),\n",
    "                'boosting_type': trial.suggest_categorical(\"lgbm_boosting_type\", ['goss', 'gbdt'])\n",
    "                }\n",
    "            classifier = LGBMClassifier(**params)  \n",
    "        \n",
    "        elif classifier_name ==\"GBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"gbm_n_estimators\", 100, 1000), \n",
    "                'criterion': trial.suggest_categorical(\"gbm_criterion\", ['squared_error', 'friedman_mse']),\n",
    "                'max_depth': trial.suggest_int(\"gbm_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_int(\"gbm_min_samples_split\", 5, 300),\n",
    "                }\n",
    "            classifier = GradientBoostingClassifier(**params)            \n",
    "        \n",
    "        else:\n",
    "            None\n",
    "        \n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"classifier\", classifier),\n",
    "            ]\n",
    "        )\n",
    "                \n",
    "        scores = cross_validate(\n",
    "                pipeline, \n",
    "                X_train,\n",
    "                y_train, \n",
    "                cv=5,\n",
    "                scoring = metric_scorer_decorated,\n",
    "                return_train_score=True)\n",
    "\n",
    "        fair_metric = scores['test_fairness'].mean()\n",
    "        model_metric = scores['test_model'].mean()\n",
    "\n",
    "        return fair_metric, model_metric\n",
    "    \n",
    "    \n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions = directions, \n",
    "        pruner = optuna.pruners.SuccessiveHalvingPruner(), \n",
    "        sampler = optuna.samplers.TPESampler() \n",
    "        )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    results.append(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object successfully saved to \"recall-fpr-models-motpe-succesivehalving-100trials-4sim.pkl\"\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "file_name = 'recall-fpr-models-motpe-succesivehalving-100trials-4sim.pkl'\n",
    "#f1-eod-lgbm-succesivehalving-30trails.pkl\n",
    "with open(file_name, 'wb') as file:\n",
    "    dill.dump(results, file)\n",
    "    print(f'Object successfully saved to \"{file_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m try_import() \u001b[39mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mplotly\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optuna\u001b[39m.\u001b[39;49mvisualization\u001b[39m.\u001b[39;49mplot_pareto_front(study, target_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mFLOPS\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/visualization/_pareto_front.py:128\u001b[0m, in \u001b[0;36mplot_pareto_front\u001b[0;34m(study, target_names, include_dominated_trials, axis_order, constraints_func, targets)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_pareto_front\u001b[39m(\n\u001b[1;32m     41\u001b[0m     study: Study,\n\u001b[1;32m     42\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     targets: Optional[Callable[[FrozenTrial], Sequence[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgo.Figure\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[39m\"\"\"Plot the Pareto front of a study.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[39m    .. seealso::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m        A :class:`plotly.graph_objs.Figure` object.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     _imports\u001b[39m.\u001b[39;49mcheck()\n\u001b[1;32m    130\u001b[0m     info \u001b[39m=\u001b[39m _get_pareto_front_info(\n\u001b[1;32m    131\u001b[0m         study, target_names, include_dominated_trials, axis_order, constraints_func, targets\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_pareto_front_plot(info)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deferred \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "optuna.visualization.plot_pareto_front(study, target_names=[\"FLOPS\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('fairenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fb7f3b855683703415b20ec6b42f145aba14d6fc5c56ba8ae1bd12772d3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
