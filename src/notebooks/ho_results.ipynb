{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fbe247-8a43-4734-84e0-64640b9a9d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    make_scorer, \n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate\n",
    "from fairlearn.metrics import (\n",
    "    count,\n",
    "    selection_rate,\n",
    "    equalized_odds_difference,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate,\n",
    "    demographic_parity_difference,\n",
    "    MetricFrame,\n",
    "    true_negative_rate,\n",
    "    true_positive_rate\n",
    ")\n",
    "\n",
    "from graphs import (\n",
    "    create_df_ranges, \n",
    "    eval_metrics_graph, \n",
    "    fig_train_test, \n",
    "    create_df_groups_metric,\n",
    "    create_df_metrics, \n",
    "    pareto_fig, \n",
    "    comparison_graph, \n",
    "    graph_eval_groups, \n",
    "    graph_eval_groups_metric,\n",
    "    indicators,\n",
    "    graph_opt_orig,\n",
    "    graph_fair_opt_orig,\n",
    "    create_df_groups_metrics\n",
    ")\n",
    "\n",
    "from metrics import (\n",
    "    equality_opportunity_difference, \n",
    "    predictive_equality_difference, \n",
    "    metric_evaluation, \n",
    "    get_metric_evaluation,\n",
    "    mse,\n",
    "    mae,\n",
    "    nmse,\n",
    "    nmae\n",
    ")\n",
    "\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from metrics import equality_opportunity_difference, predictive_equality_difference, metric_evaluation, get_metric_evaluation\n",
    "from sklearn.utils import resample\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "import dill\n",
    "import optuna\n",
    "import plotly\n",
    "import numpy as np\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9560ad6-32c7-4276-8dad-07fa4bc89cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = 'results/sex/recall-fpr-models-motpe-succesivehalving-parallel-180trials-1sim.pkl'\n",
    "with open(file_name, 'rb') as in_strm:\n",
    "    results = dill.load(in_strm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949312de-0497-4e20-9b92-97f9b70475eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "sensitive_col = 'sex'\n",
    "\n",
    "data = fetch_adult(as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == \">50K\") * 1\n",
    "\n",
    "if sensitive_col == 'race':\n",
    "    mapping = {'White':'white','Black':'black','Asian-Pac-Islander':'others','Amer-Indian-Eskimo':'others','Other':'others'}\n",
    "    X_raw.loc[:,'race'] = X_raw['race'].map(mapping).astype(\"category\")\n",
    "    \n",
    "A = X_raw[sensitive_col]\n",
    "perc = .5\n",
    "X_raw, y, A = resample(X_raw, y, A, n_samples=int(perc*X_raw.shape[0]), random_state = 123)\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LGBMClassifier(n_jobs=-1),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3808d2fb-bc30-4c61-b774-2ae41e801a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results = [results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bfdfdc8-0a1c-4b03-a32f-4dc40c05a13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "RandomForestClassifier\n",
      "GradientBoostingClassifier\n",
      "LGBMClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ooti/anaconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object successfully saved to \"<_io.BufferedWriter name='results/sex/recall-fpr-models-motpe-succesivehalving-parallel-180trials-1sim-results.pkl'>\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "metrics_sim_u = []\n",
    "metrics_sim = []\n",
    "res_sim = []\n",
    "models_sim_u = []\n",
    "models_sim = []\n",
    "y_pred_sim = []\n",
    "y_true_sim = []\n",
    "y_pred_sim_u = []\n",
    "models = [\n",
    "        #LogisticRegression(), \n",
    "          RandomForestClassifier(), \n",
    "          GradientBoostingClassifier(), \n",
    "          LGBMClassifier()\n",
    "         ]\n",
    "for sim_n,res in enumerate(results):\n",
    "    print(sim_n)\n",
    "    (X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "    X_raw, y, A, test_size=0.8, stratify=y, random_state=sim_n\n",
    "    )\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    A_train = A_train.reset_index(drop=True)\n",
    "    A_test = A_test.reset_index(drop=True)\n",
    "\n",
    "    metrics_sim_u_aux = []\n",
    "    models_sim_u_aux = []\n",
    "    y_pred_aux = []\n",
    "    for model in models:\n",
    "        print(type(model).__name__)\n",
    "        models_sim_u_aux.append(type(model).__name__)\n",
    "        pipeline.steps.pop(1)\n",
    "        pipeline.steps.insert(1,('classifier', model))\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_pred_aux.append(y_pred)\n",
    "        metrics_sim_u_aux.append(\n",
    "            metric_evaluation(\n",
    "                        y_true=y_test, \n",
    "                        y_pred=y_pred, \n",
    "                        sensitive_features=X_test[sensitive_col]\n",
    "                        )\n",
    "                )\n",
    "    y_pred_sim_u.append(y_pred_aux)    \n",
    "    metrics_sim_u.append(metrics_sim_u_aux)\n",
    "    models_sim_u.append(models_sim_u_aux)\n",
    "\n",
    "    models_sim_aux = []\n",
    "    metrics_sim_aux = []\n",
    "    res_value = []\n",
    "    y_pred_aux = []\n",
    "    for i in range(len(res.best_trials)):\n",
    "        best_params = {re.sub('^[a-z]+_','',key):value for key, value in res.best_trials[i].params.items()}\n",
    "        pipeline.steps.pop(1)\n",
    "        classifier_name = best_params.pop('classifier')\n",
    "        models_sim_aux.append(classifier_name)\n",
    "        if classifier_name == \"logit\": \n",
    "            pipeline.steps.insert(1,('classifier', LogisticRegression()))\n",
    "        elif classifier_name ==\"RF\":\n",
    "            pipeline.steps.insert(1,('classifier', RandomForestClassifier()))\n",
    "        elif classifier_name ==\"LGBM\":\n",
    "            pipeline.steps.insert(1,('classifier', LGBMClassifier()))\n",
    "        elif classifier_name ==\"GBM\":\n",
    "            pipeline.steps.insert(1,('classifier', GradientBoostingClassifier()))\n",
    "        pipeline['classifier'].set_params(**best_params)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_pred_aux.append(y_pred)\n",
    "        res_value.append((res.best_trials[i].values[0],res.best_trials[i].values[1]))\n",
    "        metrics_sim_aux.append(\n",
    "            metric_evaluation(\n",
    "                        y_true=y_test, \n",
    "                        y_pred=y_pred, \n",
    "                        sensitive_features=X_test[sensitive_col]\n",
    "                        )\n",
    "                )\n",
    "    y_pred_sim.append(y_pred_aux)\n",
    "    y_true_sim.append(y_test)\n",
    "    models_sim.append(models_sim_aux)\n",
    "    res_sim.append(res_value)\n",
    "    metrics_sim.append(metrics_sim_aux)\n",
    "\n",
    "results_dict = {}\n",
    "results_dict['res_sim'] = res_sim\n",
    "results_dict['models_sim'] = models_sim\n",
    "results_dict['metrics_sim'] = metrics_sim\n",
    "results_dict['metrics_sim_u'] = metrics_sim_u\n",
    "results_dict['models_sim_u'] = models_sim_u\n",
    "results_dict['y_pred_sim_u'] = y_pred_sim_u\n",
    "#results_dict['y_true_sim_u'] = y_true_sim_u\n",
    "results_dict['y_pred_sim'] = y_pred_sim\n",
    "results_dict['y_true_sim'] = y_true_sim\n",
    "\n",
    "file = file_name[:-4] + '-results.pkl' \n",
    "with open(file, 'wb') as file:\n",
    "    dill.dump(results_dict, file)\n",
    "    print(f'Object successfully saved to \"{file}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bce97-db53-415d-8a44-f97509687007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairenv",
   "language": "python",
   "name": "fairenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
