{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             fair_metric model_metric sensitive_attribute\n",
      "                             n_trials models [models ...] n_folds\n",
      "ipykernel_launcher.py: error: the following arguments are required: fair_metric, model_metric, sensitive_attribute, n_trials, models, n_folds\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    make_scorer, \n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    matthews_corrcoef,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "from metrics import (\n",
    "    equality_opportunity_difference,\n",
    "    predictive_equality_difference,\n",
    "    predictive_parity_difference,\n",
    "    metrics,\n",
    "    average_absolute_odds_difference\n",
    "    \n",
    ")\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.utils import resample\n",
    "import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import optuna\n",
    "import json \n",
    "import dill\n",
    "import argparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "parser = argparse.ArgumentParser() # Parser for command-line options\n",
    "parser.add_argument(\"fair_metric\", help = \"Metric to optimize fairness\", type = str)\n",
    "parser.add_argument(\"model_metric\", help = \"Metric to optimize model performance\", type = str)\n",
    "parser.add_argument(\"sensitive_attribute\", help = \"Column of the sensitive attribute\", type = str)\n",
    "parser.add_argument(\"n_trials\", help = \"Number of trials\", type = int)\n",
    "parser.add_argument('models', nargs='+', help='Models to optimize')\n",
    "parser.add_argument(\"n_folds\", help = \"Number of folds for Cross Validation\", type = int)\n",
    "\n",
    "FOLDER_ID = 'adult'\n",
    "MODEL_PATH_IN_FOLDER = 'results'\n",
    "\n",
    "args = parser.parse_args()\n",
    "fair_metric_selection = args.fair_metric\n",
    "model_metric_selection = args.model_metric\n",
    "sensitive_attribute = args.sensitive_attribute\n",
    "n_trials = args.n_trials\n",
    "models =  args.models\n",
    "n_folds = args.n_folds\n",
    "\n",
    "functions = {\n",
    "    'f1_score': f1_score,\n",
    "    'recall_score': recall_score,\n",
    "    'accuracy_score': accuracy_score,\n",
    "    'matthews_corrcoef' : matthews_corrcoef,\n",
    "    'predictive_parity_difference' : predictive_parity_difference,\n",
    "    'equality_opportunity_difference' : equality_opportunity_difference,\n",
    "    'predictive_equality_difference' : predictive_equality_difference,\n",
    "    'average_absolute_odds_difference' : average_absolute_odds_difference,\n",
    "    'demographic_parity_difference' : demographic_parity_difference,\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'r') as f:\n",
    "  metrics_dict = json.load(f)\n",
    "\n",
    "def objective_decorator(metric_scorer, X_train, y_train, models, preprocessor, n_folds = 5):\n",
    "    def objective_fn(trial):\n",
    "\n",
    "        classifier_name = trial.suggest_categorical(\"classifier\",models)\n",
    "\n",
    "        if classifier_name == \"logit\":        \n",
    "            params = {\n",
    "                \"penalty\" : trial.suggest_categorical('logit_penalty', ['l1','l2']),\n",
    "                \"C\" : trial.suggest_float('logit_c', 0.001, 10),\n",
    "                \"max_iter\": 2000,\n",
    "                \"solver\" : 'saga'\n",
    "                }\n",
    "            classifier = LogisticRegression(**params)\n",
    "\n",
    "        elif classifier_name ==\"RF\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "                'criterion': trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy']),\n",
    "                'max_depth': trial.suggest_int(\"rf_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_float(\"rf_min_samples_split\", 0.01, 1),\n",
    "                }\n",
    "            classifier = RandomForestClassifier(**params)\n",
    "\n",
    "        elif classifier_name ==\"LGBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"lgbm_n_estimators\", 20, 10000),\n",
    "                'num_leaves': trial.suggest_int(\"lgbm_num_leaves\", 10, 1000),\n",
    "                'max_depth': trial.suggest_int(\"lgbm_max_depth\", 2, 20),\n",
    "                'min_child_samples': trial.suggest_int(\"lgbm_min_child_samples\", 5, 300),\n",
    "                'learning_rate': trial.suggest_float('lgbm_learning_rate', 1e-5, 1e-2),\n",
    "                'boosting_type': trial.suggest_categorical(\"lgbm_boosting_type\", ['goss', 'gbdt'])\n",
    "                }\n",
    "            classifier = LGBMClassifier(**params)  \n",
    "\n",
    "        elif classifier_name ==\"GBM\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int(\"gbm_n_estimators\", 100, 1000), \n",
    "                'criterion': trial.suggest_categorical(\"gbm_criterion\", ['squared_error', 'friedman_mse']),\n",
    "                'max_depth': trial.suggest_int(\"gbm_max_depth\", 1, 4),\n",
    "                'min_samples_split': trial.suggest_int(\"gbm_min_samples_split\", 5, 300),\n",
    "                }\n",
    "            classifier = GradientBoostingClassifier(**params)            \n",
    "\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"classifier\", classifier),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        scores = cross_validate(\n",
    "                pipeline, \n",
    "                X_train,\n",
    "                y_train, \n",
    "                cv=n_folds,\n",
    "                scoring = metric_scorer,\n",
    "                return_train_score=True)\n",
    "\n",
    "        fair_metric = scores['test_fairness'].mean()\n",
    "        model_metric = scores['test_model'].mean()\n",
    "        return fair_metric, model_metric\n",
    "    return objective_fn\n",
    "\n",
    "def split_data(X, y, sensitive_attribute, test_size = 0.2, perc_sample = .5, sample = True, random_state = None):    \n",
    "    if sensitive_attribute == 'race':\n",
    "        mapping = {'White':'white','Black':'black','Asian-Pac-Islander':'others','Amer-Indian-Eskimo':'others','Other':'others'}\n",
    "        X.loc[:,'race'] = X['race'].map(mapping).astype(\"category\")\n",
    "    if sample:\n",
    "        X, y= resample(X, y, n_samples=int(perc_sample*X.shape[0]), random_state = random_state)\n",
    "\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(\n",
    "        X, y, test_size= test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_adult_dataset():\n",
    "    from fairlearn.datasets import fetch_adult\n",
    "    data = fetch_adult(as_frame=True)\n",
    "    X_raw = data.data\n",
    "    y = (data.target == \">50K\") * 1\n",
    "    return X_raw, y\n",
    "\n",
    "\n",
    "def save_study(study_name, study, in_dataiku = False):\n",
    "    #import dataiku\n",
    "    if in_dataiku:\n",
    "        None\n",
    "        #with dataiku.Folder(FOLDER_ID).get_writer(MODEL_PATH_IN_FOLDER) as writer:\n",
    "        #    writeable = dill.dumps(study)\n",
    "        #writer.write(writeable)\n",
    "    else:\n",
    "        file_name = study_name +'.pkl'\n",
    "        with open(file_name, 'wb') as file:\n",
    "            dill.dump(study, file)\n",
    "            print(f'Object successfully saved to \"{file_name}\"')\n",
    "\n",
    "#def fair_ho_optimize(X_train, y_train, models, fair_metric, model_metric, sensitive_attribute, directions, n_trials):\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    fair_metric = functions[fair_metric_selection]\n",
    "    model_metric = functions[model_metric_selection]\n",
    "    fair_direction = metrics_dict[fair_metric_selection]['optimization']\n",
    "    model_direction = metrics_dict[model_metric_selection]['optimization']\n",
    "    directions = [fair_direction, model_direction]\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"impute\", SimpleImputer()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "    categorical_transformer = Pipeline(\n",
    "        [\n",
    "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X_raw, y = get_adult_dataset()\n",
    "    X_train, X_test, y_train, y_test = split_data(X_raw, y, sensitive_attribute = sensitive_attribute)\n",
    "    metric_scorer_decorated =  metrics(fair_metric,model_metric, sensitive_attribute)\n",
    "    objective = objective_decorator(metric_scorer_decorated, X_train, y_train, models)\n",
    "    study = optuna.create_study(\n",
    "        directions = directions, \n",
    "        pruner = pruner, \n",
    "        sampler = sampler,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-12 11:21:07,175] A new study created in memory with name: no-name-b1755347-aab0-4354-b088-9abc424c0e53\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2023-07-12 11:21:11,967] Trial 0 finished with values: [0.0, 0.0] and parameters: {'classifier': 'RF', 'rf_n_estimators': 225, 'rf_criterion': 'entropy', 'rf_max_depth': 1, 'rf_min_samples_split': 0.24684683768186794}. \n",
      "[W 2023-07-12 11:21:43,741] Trial 1 failed with parameters: {'classifier': 'LGBM', 'lgbm_n_estimators': 9618, 'lgbm_num_leaves': 456, 'lgbm_max_depth': 14, 'lgbm_min_child_samples': 265, 'lgbm_learning_rate': 0.006162350104163533, 'lgbm_boosting_type': 'gbdt'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_59068/1975492859.py\", line 54, in objective_fn\n",
      "    scores = cross_validate(\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 711, in _fit_and_score\n",
      "    train_scores = _score(estimator, X_train, y_train, scorer, error_score)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/azucena/fairness/src/notebooks/metrics.py\", line 73, in metric_scorer\n",
      "    y_pred = clf.predict(X)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 984, in predict\n",
      "    result = self.predict_proba(X, raw_score, start_iteration, num_iteration,\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 997, in predict_proba\n",
      "    result = super().predict(X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 803, in predict\n",
      "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py\", line 3538, in predict\n",
      "    return predictor.predict(data, start_iteration, num_iteration,\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py\", line 844, in predict\n",
      "    preds, nrow = self.__pred_for_csr(data, start_iteration, num_iteration, predict_type)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py\", line 1077, in __pred_for_csr\n",
      "    return inner_predict(csr, start_iteration, num_iteration, predict_type)\n",
      "  File \"/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py\", line 1003, in inner_predict\n",
      "    _safe_call(_LIB.LGBM_BoosterPredictForCSR(\n",
      "KeyboardInterrupt\n",
      "[W 2023-07-12 11:21:43,753] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m#storage_name = \"sqlite:///{}.db\".format(study_name)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m      4\u001b[0m     directions \u001b[39m=\u001b[39m directions, \n\u001b[1;32m      5\u001b[0m     pruner \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mSuccessiveHalvingPruner(), \n\u001b[1;32m      6\u001b[0m     sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(),\n\u001b[1;32m      7\u001b[0m     \u001b[39m#storage = storage_name\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mn_trials, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[21], line 54\u001b[0m, in \u001b[0;36mobjective_decorator.<locals>.objective_fn\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(\n\u001b[1;32m     48\u001b[0m     steps\u001b[39m=\u001b[39m[\n\u001b[1;32m     49\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m, preprocessor),\n\u001b[1;32m     50\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m, classifier),\n\u001b[1;32m     51\u001b[0m     ]\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 54\u001b[0m scores \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m     55\u001b[0m         pipeline, \n\u001b[1;32m     56\u001b[0m         X_train,\n\u001b[1;32m     57\u001b[0m         y_train, \n\u001b[1;32m     58\u001b[0m         cv\u001b[39m=\u001b[39;49mn_folds,\n\u001b[1;32m     59\u001b[0m         scoring \u001b[39m=\u001b[39;49m metric_scorer,\n\u001b[1;32m     60\u001b[0m         return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m fair_metric \u001b[39m=\u001b[39m scores[\u001b[39m'\u001b[39m\u001b[39mtest_fairness\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     63\u001b[0m model_metric \u001b[39m=\u001b[39m scores[\u001b[39m'\u001b[39m\u001b[39mtest_model\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:711\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m     score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[1;32m    710\u001b[0m     \u001b[39mif\u001b[39;00m return_train_score:\n\u001b[0;32m--> 711\u001b[0m         train_scores \u001b[39m=\u001b[39m _score(estimator, X_train, y_train, scorer, error_score)\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    714\u001b[0m     total_time \u001b[39m=\u001b[39m score_time \u001b[39m+\u001b[39m fit_time\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[1;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    770\u001b[0m         \u001b[39m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    771\u001b[0m         \u001b[39m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m~/fairness/src/notebooks/metrics.py:73\u001b[0m, in \u001b[0;36mmetrics.<locals>.metric_scorer\u001b[0;34m(clf, X, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmetric_scorer\u001b[39m(clf, X, y):\n\u001b[0;32m---> 73\u001b[0m     y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m     74\u001b[0m     performance_metric \u001b[39m=\u001b[39m model_metric(y,y_pred)\n\u001b[1;32m     75\u001b[0m     fairness_metric \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(fair_metric(y, y_pred, sensitive_features \u001b[39m=\u001b[39m X[sensitive_col]))\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    480\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpredict(Xt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpredict_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py:984\u001b[0m, in \u001b[0;36mLGBMClassifier.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X, raw_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, start_iteration\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    982\u001b[0m             pred_leaf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pred_contrib\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    983\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X, raw_score, start_iteration, num_iteration,\n\u001b[1;32m    985\u001b[0m                                 pred_leaf, pred_contrib, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    986\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objective) \u001b[39mor\u001b[39;00m raw_score \u001b[39mor\u001b[39;00m pred_leaf \u001b[39mor\u001b[39;00m pred_contrib:\n\u001b[1;32m    987\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py:997\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X, raw_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, start_iteration\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    995\u001b[0m                   pred_leaf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pred_contrib\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    996\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    998\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objective) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (raw_score \u001b[39mor\u001b[39;00m pred_leaf \u001b[39mor\u001b[39;00m pred_contrib):\n\u001b[1;32m    999\u001b[0m         _log_warning(\u001b[39m\"\u001b[39m\u001b[39mCannot compute class probabilities or labels \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1000\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mdue to the usage of customized objective function.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1001\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mReturning raw scores instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/sklearn.py:803\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features \u001b[39m!=\u001b[39m n_features:\n\u001b[1;32m    800\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of features of the model must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    801\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatch the input. Model n_features_ is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput n_features is \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 803\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Booster\u001b[39m.\u001b[39;49mpredict(X, raw_score\u001b[39m=\u001b[39;49mraw_score, start_iteration\u001b[39m=\u001b[39;49mstart_iteration, num_iteration\u001b[39m=\u001b[39;49mnum_iteration,\n\u001b[1;32m    804\u001b[0m                              pred_leaf\u001b[39m=\u001b[39;49mpred_leaf, pred_contrib\u001b[39m=\u001b[39;49mpred_contrib, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3537\u001b[0m         num_iteration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 3538\u001b[0m \u001b[39mreturn\u001b[39;00m predictor\u001b[39m.\u001b[39;49mpredict(data, start_iteration, num_iteration,\n\u001b[1;32m   3539\u001b[0m                          raw_score, pred_leaf, pred_contrib,\n\u001b[1;32m   3540\u001b[0m                          data_has_header, is_reshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py:844\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    842\u001b[0m         nrow \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    843\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcsr_matrix):\n\u001b[0;32m--> 844\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pred_for_csr(data, start_iteration, num_iteration, predict_type)\n\u001b[1;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, scipy\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mcsc_matrix):\n\u001b[1;32m    846\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py:1077\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_csr\u001b[0;34m(self, csr, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[39mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1076\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_predict(csr, start_iteration, num_iteration, predict_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/fairenv/lib/python3.10/site-packages/lightgbm/basic.py:1003\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_csr.<locals>.inner_predict\u001b[0;34m(csr, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[39massert\u001b[39;00m csr\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m MAX_INT32\n\u001b[1;32m   1001\u001b[0m csr_indices \u001b[39m=\u001b[39m csr\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1003\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterPredictForCSR(\n\u001b[1;32m   1004\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1005\u001b[0m     ptr_indptr,\n\u001b[1;32m   1006\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_indptr),\n\u001b[1;32m   1007\u001b[0m     csr_indices\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_int32)),\n\u001b[1;32m   1008\u001b[0m     ptr_data,\n\u001b[1;32m   1009\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m   1010\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int64(\u001b[39mlen\u001b[39;49m(csr\u001b[39m.\u001b[39;49mindptr)),\n\u001b[1;32m   1011\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int64(\u001b[39mlen\u001b[39;49m(csr\u001b[39m.\u001b[39;49mdata)),\n\u001b[1;32m   1012\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int64(csr\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m   1013\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(predict_type),\n\u001b[1;32m   1014\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(start_iteration),\n\u001b[1;32m   1015\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(num_iteration),\n\u001b[1;32m   1016\u001b[0m     c_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_parameter),\n\u001b[1;32m   1017\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_num_preds),\n\u001b[1;32m   1018\u001b[0m     preds\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_double))))\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m n_preds \u001b[39m!=\u001b[39m out_num_preds\u001b[39m.\u001b[39mvalue:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length for predict results\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
