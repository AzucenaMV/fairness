{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tqdm as notebook_tqdm\n",
    "from metrics import (\n",
    "    equality_opportunity_difference,\n",
    "    predictive_equality_difference,\n",
    "    predictive_parity_difference,\n",
    "    metrics,\n",
    "    average_absolute_odds_difference,\n",
    "    metric_evaluation, \n",
    "    get_metric_evaluation,\n",
    "    \n",
    ")\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    confusion_matrix, \n",
    "    make_scorer, \n",
    "    accuracy_score, \n",
    "    recall_score, \n",
    "    matthews_corrcoef,\n",
    "    precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective(trial, data_dict, sensitive_col, models, preprocessor):\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", models)\n",
    "\n",
    "    if classifier_name == \"logit\":        \n",
    "        params = {\n",
    "            \"penalty\" : trial.suggest_categorical('logit_penalty', ['l1','l2']),\n",
    "            \"C\" : trial.suggest_float('logit_c', 0.001, 10),\n",
    "            \"max_iter\": 2000,\n",
    "            \"solver\" : 'saga'\n",
    "            }\n",
    "        classifier = LogisticRegression(**params)\n",
    "\n",
    "    elif classifier_name ==\"RF\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            'criterion': trial.suggest_categorical(\"rf_criterion\", ['gini', 'entropy']),\n",
    "            'max_depth': trial.suggest_int(\"rf_max_depth\", 1, 4),\n",
    "            'min_samples_split': trial.suggest_float(\"rf_min_samples_split\", 0.01, 1),\n",
    "            }\n",
    "        classifier = RandomForestClassifier(**params)\n",
    "\n",
    "    elif classifier_name ==\"LGBM\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int(\"lgbm_n_estimators\", 20, 10000),\n",
    "            'num_leaves': trial.suggest_int(\"lgbm_num_leaves\", 10, 1000),\n",
    "            'max_depth': trial.suggest_int(\"lgbm_max_depth\", 2, 20),\n",
    "            'min_child_samples': trial.suggest_int(\"lgbm_min_child_samples\", 5, 300),\n",
    "            'learning_rate': trial.suggest_float('lgbm_learning_rate', 1e-5, 1e-2),\n",
    "            'boosting_type': trial.suggest_categorical(\"lgbm_boosting_type\", ['goss', 'gbdt'])\n",
    "            }\n",
    "        classifier = LGBMClassifier(**params)  \n",
    "\n",
    "    elif classifier_name ==\"GBM\":\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int(\"gbm_n_estimators\", 100, 1000), \n",
    "            'criterion': trial.suggest_categorical(\"gbm_criterion\", ['squared_error', 'friedman_mse']),\n",
    "            'max_depth': trial.suggest_int(\"gbm_max_depth\", 1, 4),\n",
    "            'min_samples_split': trial.suggest_int(\"gbm_min_samples_split\", 5, 300),\n",
    "            }\n",
    "        classifier = GradientBoostingClassifier(**params)            \n",
    "\n",
    "    else:\n",
    "        None\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", classifier),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(data_dict['X_train'], data_dict['y_train'])\n",
    "    y_pred = pipeline.predict(data_dict['X_test'])\n",
    "    metrics = metric_evaluation(\n",
    "        y_true= data_dict['y_test'], \n",
    "        y_pred= y_pred, \n",
    "        sensitive_features=data_dict['X_test'][sensitive_col]\n",
    "        )\n",
    "    return classifier_name, metrics\n",
    "\n",
    "\n",
    "def get_default_metrics(metrics, data_dict, sensitive_col, preprocessor):\n",
    "    models = metrics['overall']['model_name'].unique()\n",
    "    classifier = {\n",
    "        'logit' : LogisticRegression(),\n",
    "        'GBM' : GradientBoostingClassifier(),\n",
    "        'LGBM' : LGBMClassifier(),\n",
    "        'RF' : RandomForestClassifier(),\n",
    "    }\n",
    "\n",
    "    metrics['default_overall'] = pd.DataFrame()\n",
    "    metrics['default_bygroup'] = pd.DataFrame()\n",
    "    for model in models:\n",
    "        clf = classifier[model]\n",
    "        pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"classifier\", clf),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        pipeline.fit(data_dict['X_train'], data_dict['y_train'])\n",
    "        y_pred = pipeline.predict(data_dict['X_test'])\n",
    "        metric_frame = metric_evaluation(\n",
    "            y_true= data_dict['y_test'], \n",
    "            y_pred=y_pred, \n",
    "            sensitive_features=data_dict['X_test'][sensitive_col]\n",
    "        )\n",
    "        # Overall\n",
    "        fair_records = pd.DataFrame.from_records([get_metric_evaluation(metric_frame)])\n",
    "        new_metric_overall = pd.concat([fair_records, pd.DataFrame(metric_frame.overall).T], axis = 1)\n",
    "        new_metric_overall['model'] = model\n",
    "        metrics['default_overall'] = pd.concat([metrics['default_overall'], new_metric_overall])\n",
    "        # By group\n",
    "        new_metric_bygroup = metric_frame.by_group.reset_index()\n",
    "        new_metric_bygroup['model'] = model\n",
    "        metrics['default_bygroup'] = pd.concat([metrics['default_bygroup'], new_metric_bygroup])\n",
    "    return metrics\n",
    "\n",
    "def get_metrics(study, data_dict, sensitive_col, models, preprocessor):\n",
    "    metrics = {}\n",
    "    metrics['overall'] = pd.DataFrame()\n",
    "    metrics['bygroup'] = pd.DataFrame()\n",
    "    try:\n",
    "        metrics['fair_metric'] = study.user_attrs['fair_metric']\n",
    "        metrics['model_metric'] = study.user_attrs['model_metric']\n",
    "    except:\n",
    "        print('User attributes not found')\n",
    "    i = 1\n",
    "    for best_trial in study.best_trials:\n",
    "        if best_trial.values != [0,0]:\n",
    "            fair_value, model_value = best_trial.values\n",
    "            clf_name, metric = detailed_objective(best_trial, data_dict, sensitive_col, models, preprocessor)\n",
    "            # Overall\n",
    "            fair_records = pd.DataFrame.from_records([get_metric_evaluation(metric)])\n",
    "            new_metric_overall = pd.concat([fair_records, pd.DataFrame(metric.overall).T], axis = 1)\n",
    "            new_metric_overall['best_trial'] = i\n",
    "            new_metric_overall['fair_metric'] = fair_value\n",
    "            new_metric_overall['model_metric'] = model_value\n",
    "            new_metric_overall['model_name'] = clf_name\n",
    "            metrics['overall'] = pd.concat([metrics['overall'], new_metric_overall])\n",
    "            # By Groups\n",
    "            new_metric_bygroup = metric.by_group.reset_index()\n",
    "            new_metric_bygroup['best_trial'] = i\n",
    "            metrics['bygroup'] = pd.concat([metrics['bygroup'], new_metric_bygroup])\n",
    "            i += 1\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/azucena/miniconda3/envs/fairenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "n_sim = 123\n",
    "sensitive_col = 'sex'\n",
    "sensitive_attribute = 'sex'\n",
    "models =  [\"GBM\",\"LGBM\",\"RF\"]\n",
    "file_name = 'results/recall_score_demographic_parity_difference_100_20230724151855.pkl'\n",
    "\n",
    "with open(file_name, 'rb') as in_strm:\n",
    "    study = dill.load(in_strm)\n",
    "\n",
    "data = fetch_adult(as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == \">50K\") * 1\n",
    "\n",
    "if sensitive_attribute == 'race':\n",
    "    mapping = {'White':'white','Black':'black','Asian-Pac-Islander':'others','Amer-Indian-Eskimo':'others','Other':'others'}\n",
    "    X_raw.loc[:,'race'] = X_raw['race'].map(mapping).astype(\"category\")\n",
    "\n",
    "perc = .5\n",
    "X_raw, y = resample(X_raw, y, n_samples=int(perc*X_raw.shape[0]), random_state = n_sim)  \n",
    "  \n",
    "(X_train, X_test, y_train, y_test) = train_test_split(\n",
    "    X_raw, y, test_size=0.8, stratify=y, random_state=n_sim\n",
    ")\n",
    "\n",
    "data_dict = {}\n",
    "data_dict['X_train'] = X_train.reset_index(drop=True)\n",
    "data_dict['X_test'] = X_test.reset_index(drop=True)\n",
    "data_dict['y_train'] = y_train.reset_index(drop=True)\n",
    "data_dict['y_test'] = y_test.reset_index(drop=True)\n",
    "\n",
    "metrics = get_metrics(study, data_dict, sensitive_col, models, preprocessor)\n",
    "metrics = get_default_metrics(metrics, data_dict, sensitive_col, preprocessor)\n",
    "metrics['file_name'] = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall':    demographic parity  predictive parity  equality opportunity   \n",
       " 0            0.152969           0.067696              0.105239  \\\n",
       " 0            0.187880           0.008934              0.075670   \n",
       " 0            0.187071           0.001063              0.078167   \n",
       " 0            0.033137           0.001903              0.032304   \n",
       " 0            0.144394           0.173891              0.337491   \n",
       " 0            0.153196           0.067009              0.107707   \n",
       " 0            0.161669           0.057531              0.085767   \n",
       " 0            0.160157           0.061351              0.095634   \n",
       " 0            0.190324           0.020519              0.090863   \n",
       " 0            0.158695           0.062478              0.089621   \n",
       " 0            0.165751           0.055439              0.100390   \n",
       " 0            0.140366           0.167883              0.336126   \n",
       " 0            0.020599           0.996255              0.067359   \n",
       " 0            0.025073           0.996923              0.082046   \n",
       " 0            0.160699           0.065160              0.093926   \n",
       " 0            0.189094           0.012641              0.069605   \n",
       " 0            0.023530           0.996721              0.076982   \n",
       " 0            0.011727           0.993421              0.038238   \n",
       " 0            0.186992           0.022068              0.060704   \n",
       " 0            0.166640           0.055990              0.087197   \n",
       " 0            0.020676           0.996269              0.067612   \n",
       " 0            0.169042           0.071249              0.079669   \n",
       " 0            0.170856           0.041015              0.090781   \n",
       " 0            0.018901           0.995918              0.061788   \n",
       " 0            0.024842           0.996894              0.081286   \n",
       " 0            0.050323           0.032389              0.120340   \n",
       " 0            0.174088           0.031818              0.081262   \n",
       " 0            0.157906           0.054524              0.098574   \n",
       " 0            0.042751           0.008865              0.134752   \n",
       " 0            0.032788           0.990588              0.106609   \n",
       " 0            0.030114           0.008282              0.057351   \n",
       " 0            0.166487           0.057330              0.085077   \n",
       " 0            0.020907           0.996310              0.068372   \n",
       " \n",
       "    predictive equality  average absolute odds  accuracy  precision    recall   \n",
       " 0             0.054550               0.079895  0.854686   0.797293  0.528181  \\\n",
       " 0             0.084355               0.080012  0.852741   0.713442  0.644748   \n",
       " 0             0.083414               0.080791  0.853611   0.718600  0.640051   \n",
       " 0             0.000716               0.016510  0.794697   0.986975  0.145602   \n",
       " 0             0.038278               0.187884  0.829401   0.830965  0.362084   \n",
       " 0             0.054550               0.081129  0.854532   0.797097  0.527541   \n",
       " 0             0.058144               0.071956  0.862927   0.795000  0.577071   \n",
       " 0             0.057447               0.076541  0.860675   0.795660  0.563621   \n",
       " 0             0.079028               0.084946  0.864258   0.746124  0.657558   \n",
       " 0             0.057225               0.073423  0.860214   0.795461  0.561272   \n",
       " 0             0.058315               0.079353  0.865281   0.797909  0.586678   \n",
       " 0             0.035726               0.185926  0.828019   0.836382  0.351409   \n",
       " 0             0.000111               0.033735  0.773814   0.996255  0.056789   \n",
       " 0             0.000111               0.041079  0.776783   0.996923  0.069172   \n",
       " 0             0.058173               0.076050  0.860777   0.795072  0.564902   \n",
       " 0             0.078134               0.073870  0.866510   0.745855  0.672289   \n",
       " 0             0.000111               0.038546  0.775759   0.996721  0.064902   \n",
       " 0             0.000111               0.019174  0.767928   0.993421  0.032237   \n",
       " 0             0.084106               0.072405  0.857194   0.727207  0.647096   \n",
       " 0             0.061555               0.074376  0.863797   0.788259  0.590521   \n",
       " 0             0.000111               0.033862  0.773865   0.996269  0.057003   \n",
       " 0             0.064297               0.071983  0.864923   0.787623  0.597780   \n",
       " 0             0.061618               0.076199  0.867431   0.788271  0.611230   \n",
       " 0             0.000111               0.030949  0.772688   0.995918  0.052092   \n",
       " 0             0.000111               0.040699  0.776629   0.996894  0.068531   \n",
       " 0             0.002663               0.061502  0.798024   0.969466  0.162681   \n",
       " 0             0.065535               0.073398  0.866510   0.775624  0.623612   \n",
       " 0             0.055612               0.077093  0.859702   0.797004  0.556576   \n",
       " 0             0.000555               0.067653  0.788862   0.991213  0.120410   \n",
       " 0             0.000444               0.053527  0.781594   0.990588  0.089880   \n",
       " 0             0.000444               0.028897  0.786968   0.992453  0.112297   \n",
       " 0             0.061888               0.073482  0.863541   0.787464  0.590094   \n",
       " 0             0.000111               0.034241  0.774019   0.996310  0.057643   \n",
       " \n",
       "    f1 score       mcc  selection rate  false positive rate   \n",
       " 0  0.635418  0.567466        0.158827             0.042348  \\\n",
       " 0  0.677358  0.583526        0.216666             0.081667   \n",
       " 0  0.677055  0.584450        0.213544             0.079041   \n",
       " 0  0.253767  0.335138        0.035369             0.000606   \n",
       " 0  0.504387  0.472977        0.104468             0.023228   \n",
       " 0  0.634892  0.566940        0.158673             0.042348   \n",
       " 0  0.668728  0.596979        0.174029             0.046927   \n",
       " 0  0.659835  0.588942        0.169832             0.045647   \n",
       " 0  0.699047  0.613898        0.211291             0.070558   \n",
       " 0  0.658155  0.587342        0.169166             0.045513   \n",
       " 0  0.676181  0.604802        0.176281             0.046859   \n",
       " 0  0.494889  0.467722        0.100732             0.021679   \n",
       " 0  0.107453  0.208578        0.013666             0.000067   \n",
       " 0  0.129367  0.230671        0.016635             0.000067   \n",
       " 0  0.660509  0.589387        0.170343             0.045917   \n",
       " 0  0.707164  0.622420        0.216103             0.072241   \n",
       " 0  0.121868  0.223285        0.015611             0.000067   \n",
       " 0  0.062448  0.156319        0.007780             0.000067   \n",
       " 0  0.684817  0.594593        0.213339             0.076550   \n",
       " 0  0.675211  0.601143        0.179608             0.050024   \n",
       " 0  0.107835  0.208978        0.013718             0.000067   \n",
       " 0  0.679694  0.605238        0.181962             0.050831   \n",
       " 0  0.688552  0.613962        0.185904             0.051774   \n",
       " 0  0.099006  0.199598        0.012540             0.000067   \n",
       " 0  0.128246  0.229577        0.016482             0.000067   \n",
       " 0  0.278611  0.349941        0.040231             0.001616   \n",
       " 0  0.691361  0.613360        0.192762             0.056891   \n",
       " 0  0.655437  0.585323        0.167426             0.044705   \n",
       " 0  0.214734  0.304856        0.029124             0.000337   \n",
       " 0  0.164807  0.262259        0.021754             0.000269   \n",
       " 0  0.201764  0.294406        0.027128             0.000269   \n",
       " 0  0.674640  0.600377        0.179659             0.050226   \n",
       " 0  0.108981  0.210172        0.013871             0.000067   \n",
       " \n",
       "    true positive rate  false negative rate  true negative rate    count   \n",
       " 0            0.528181             0.471819            0.957652  19537.0  \\\n",
       " 0            0.644748             0.355252            0.918333  19537.0   \n",
       " 0            0.640051             0.359949            0.920959  19537.0   \n",
       " 0            0.145602             0.854398            0.999394  19537.0   \n",
       " 0            0.362084             0.637916            0.976772  19537.0   \n",
       " 0            0.527541             0.472459            0.957652  19537.0   \n",
       " 0            0.577071             0.422929            0.953073  19537.0   \n",
       " 0            0.563621             0.436379            0.954353  19537.0   \n",
       " 0            0.657558             0.342442            0.929442  19537.0   \n",
       " 0            0.561272             0.438728            0.954487  19537.0   \n",
       " 0            0.586678             0.413322            0.953141  19537.0   \n",
       " 0            0.351409             0.648591            0.978321  19537.0   \n",
       " 0            0.056789             0.943211            0.999933  19537.0   \n",
       " 0            0.069172             0.930828            0.999933  19537.0   \n",
       " 0            0.564902             0.435098            0.954083  19537.0   \n",
       " 0            0.672289             0.327711            0.927759  19537.0   \n",
       " 0            0.064902             0.935098            0.999933  19537.0   \n",
       " 0            0.032237             0.967763            0.999933  19537.0   \n",
       " 0            0.647096             0.352904            0.923450  19537.0   \n",
       " 0            0.590521             0.409479            0.949976  19537.0   \n",
       " 0            0.057003             0.942997            0.999933  19537.0   \n",
       " 0            0.597780             0.402220            0.949169  19537.0   \n",
       " 0            0.611230             0.388770            0.948226  19537.0   \n",
       " 0            0.052092             0.947908            0.999933  19537.0   \n",
       " 0            0.068531             0.931469            0.999933  19537.0   \n",
       " 0            0.162681             0.837319            0.998384  19537.0   \n",
       " 0            0.623612             0.376388            0.943109  19537.0   \n",
       " 0            0.556576             0.443424            0.955295  19537.0   \n",
       " 0            0.120410             0.879590            0.999663  19537.0   \n",
       " 0            0.089880             0.910120            0.999731  19537.0   \n",
       " 0            0.112297             0.887703            0.999731  19537.0   \n",
       " 0            0.590094             0.409906            0.949774  19537.0   \n",
       " 0            0.057643             0.942357            0.999933  19537.0   \n",
       " \n",
       "    best_trial  fair_metric  model_metric model_name  \n",
       " 0           1     0.155004      0.546969        GBM  \n",
       " 0           2     0.183918      0.730572       LGBM  \n",
       " 0           3     0.178230      0.706446       LGBM  \n",
       " 0           4     0.047478      0.248936       LGBM  \n",
       " 0           5     0.110999      0.305293         RF  \n",
       " 0           6     0.154452      0.545688        GBM  \n",
       " 0           7     0.166155      0.589882        GBM  \n",
       " 0           8     0.162997      0.574723        GBM  \n",
       " 0           9     0.186306      0.731855       LGBM  \n",
       " 0          10     0.162373      0.572801        GBM  \n",
       " 0          11     0.173962      0.609735        GBM  \n",
       " 0          12     0.103702      0.287786         RF  \n",
       " 0          13     0.014120      0.038856         RF  \n",
       " 0          14     0.027102      0.074937         RF  \n",
       " 0          15     0.163763      0.578139        GBM  \n",
       " 0          16     0.183018      0.729505       LGBM  \n",
       " 0          17     0.024400      0.067463         RF  \n",
       " 0          18     0.006659      0.018360         RF  \n",
       " 0          19     0.182591      0.725236       LGBM  \n",
       " 0          20     0.168987      0.601836        GBM  \n",
       " 0          21     0.009271      0.025619         RF  \n",
       " 0          22     0.170415      0.607174        GBM  \n",
       " 0          23     0.177238      0.635782        GBM  \n",
       " 0          24     0.011177      0.030954         RF  \n",
       " 0          25     0.022147      0.061275         RF  \n",
       " 0          26     0.045627      0.150944         RF  \n",
       " 0          27     0.177775      0.644535        GBM  \n",
       " 0          28     0.161110      0.568319        GBM  \n",
       " 0          29     0.031987      0.088600         RF  \n",
       " 0          30     0.029895      0.082408         RF  \n",
       " 0          31     0.033215      0.094149         RF  \n",
       " 0          32     0.168819      0.601196        GBM  \n",
       " 0          33     0.014489      0.040137         RF  ,\n",
       " 'bygroup':        sex  accuracy  precision    recall  f1 score       mcc  selection rate   \n",
       " 0   Female  0.929125   0.856764  0.439456  0.580935  0.583087        0.057338  \\\n",
       " 1     Male  0.816926   0.789068  0.544695  0.644494  0.543129        0.210307   \n",
       " 0   Female  0.926084   0.705785  0.580952  0.637313  0.600097        0.092015   \n",
       " 1     Male  0.815538   0.714719  0.656622  0.684440  0.555444        0.279895   \n",
       " 0   Female  0.927148   0.717687  0.574150  0.637944  0.602602        0.089430   \n",
       " ..     ...       ...        ...       ...       ...       ...             ...   \n",
       " 1     Male  0.731986   0.991718  0.121297  0.216155  0.293678        0.037263   \n",
       " 0   Female  0.934905   0.837363  0.518367  0.640336  0.627853        0.069202   \n",
       " 1     Male  0.827341   0.780033  0.603444  0.680468  0.573539        0.235689   \n",
       " 0   Female  0.888213   0.000000  0.000000  0.000000  0.000000        0.000000   \n",
       " 1     Male  0.716093   0.996310  0.068372  0.127962  0.219592        0.020907   \n",
       " \n",
       "     false positive rate  true positive rate  false negative rate   \n",
       " 0              0.009247            0.439456             0.560544  \\\n",
       " 1              0.063797            0.544695             0.455305   \n",
       " 0              0.030479            0.580952             0.419048   \n",
       " 1              0.114834            0.656622             0.343378   \n",
       " 0              0.028425            0.574150             0.425850   \n",
       " ..                  ...                 ...                  ...   \n",
       " 1              0.000444            0.121297             0.878703   \n",
       " 0              0.012671            0.518367             0.481633   \n",
       " 1              0.074559            0.603444             0.396556   \n",
       " 0              0.000000            0.000000             1.000000   \n",
       " 1              0.000111            0.068372             0.931628   \n",
       " \n",
       "     true negative rate    count  best_trial  \n",
       " 0             0.990753   6575.0           1  \n",
       " 1             0.936203  12962.0           1  \n",
       " 0             0.969521   6575.0           2  \n",
       " 1             0.885166  12962.0           2  \n",
       " 0             0.971575   6575.0           3  \n",
       " ..                 ...      ...         ...  \n",
       " 1             0.999556  12962.0          31  \n",
       " 0             0.987329   6575.0          32  \n",
       " 1             0.925441  12962.0          32  \n",
       " 0             1.000000   6575.0          33  \n",
       " 1             0.999889  12962.0          33  \n",
       " \n",
       " [66 rows x 13 columns],\n",
       " 'fair_metric': 'demographic_parity_difference',\n",
       " 'model_metric': 'recall_score',\n",
       " 'default_overall':    demographic parity  predictive parity  equality opportunity   \n",
       " 0            0.164365           0.057861              0.094630  \\\n",
       " 0            0.178084           0.005501              0.075580   \n",
       " 0            0.183672           0.017443              0.083867   \n",
       " \n",
       "    predictive equality  average absolute odds  accuracy  precision    recall   \n",
       " 0             0.058043               0.076337  0.865025   0.798484  0.584543  \\\n",
       " 0             0.064868               0.070224  0.870246   0.768290  0.656917   \n",
       " 0             0.072571               0.078219  0.866407   0.758089  0.650299   \n",
       " \n",
       "    f1 score       mcc  selection rate  false positive rate   \n",
       " 0  0.674966  0.603823        0.175513             0.046523  \\\n",
       " 0  0.708252  0.628648        0.204996             0.062479   \n",
       " 0  0.700069  0.617773        0.205661             0.065441   \n",
       " \n",
       "    true positive rate  false negative rate  true negative rate    count model  \n",
       " 0            0.584543             0.415457            0.953477  19537.0   GBM  \n",
       " 0            0.656917             0.343083            0.937521  19537.0  LGBM  \n",
       " 0            0.650299             0.349701            0.934559  19537.0    RF  ,\n",
       " 'default_bygroup':       sex  accuracy  precision    recall  f1 score       mcc  selection rate   \n",
       " 0  Female  0.934601   0.848970  0.504762  0.633106  0.624235        0.066464  \\\n",
       " 1    Male  0.829733   0.791110  0.599392  0.682034  0.578982        0.230829   \n",
       " 0  Female  0.933992   0.763573  0.593197  0.667688  0.637893        0.086844   \n",
       " 1    Male  0.837911   0.769074  0.668777  0.715427  0.605757        0.264928   \n",
       " 0  Female  0.933992   0.773140  0.579592  0.662519  0.634764        0.083802   \n",
       " 1    Male  0.832125   0.755697  0.663459  0.706580  0.592155        0.267474   \n",
       " \n",
       "    false positive rate  true positive rate  false negative rate   \n",
       " 0             0.011301            0.504762             0.495238  \\\n",
       " 1             0.069344            0.599392             0.400608   \n",
       " 0             0.023116            0.593197             0.406803   \n",
       " 1             0.087984            0.668777             0.331223   \n",
       " 0             0.021404            0.579592             0.420408   \n",
       " 1             0.093975            0.663459             0.336541   \n",
       " \n",
       "    true negative rate    count model  \n",
       " 0            0.988699   6575.0   GBM  \n",
       " 1            0.930656  12962.0   GBM  \n",
       " 0            0.976884   6575.0  LGBM  \n",
       " 1            0.912016  12962.0  LGBM  \n",
       " 0            0.978596   6575.0    RF  \n",
       " 1            0.906025  12962.0    RF  ,\n",
       " 'file_name': 'results/recall_score_demographic_parity_difference_100_20230724151855.pkl'}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/recall_score_demographic_parity_difference_100_20230724151855-metrics.pkl'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'results/recall_score_demographic_parity_difference_100_20230724151855.pkl'\n",
    "file_name = file_name[:-4] + '-metrics.pkl'\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "        dif = metrics['bygroup'][metrics['bygroup'].best_trial == 5].apply(pd.to_numeric, errors='coerce').diff().abs().iloc[-1,:]\n",
    "        #dif[0] = 'Difference'\n",
    "        df_groups_m = metrics['bygroup'][metrics['bygroup'].best_trial == 5].T\n",
    "        df_groups_m = pd.concat([df_groups_m,dif], axis = 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_col = 'demographic parity'\n",
    "model_col = 'recall'\n",
    "df_default_overall = metrics['default_overall'].rename(columns = {'model':'model_name'})\n",
    "df_optimized_overall = metrics['overall']\n",
    "df_overall = pd.concat([df_optimized_overall,df_default_overall])\n",
    "df_overall = df_overall.sort_values([fair_col]).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair metric'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_col = 'fair_metric'\n",
    "train_col.replace('_',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object successfully saved to \"results/recall_score_demographic_parity_difference_100_20230724151855-metrics.pkl\"\n"
     ]
    }
   ],
   "source": [
    "with open(file_name, 'wb') as file:\n",
    "    dill.dump(metrics, file)\n",
    "    print(f'Object successfully saved to \"{file_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = metrics['overall'][metrics['overall'].index == 5].model_name\n",
    "n_model = metrics['default_bygroup']['model'].isin(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['overall', 'bygroup', 'fair_metric', 'model_metric', 'default_overall', 'default_bygroup', 'file_name'])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demographic parity</th>\n",
       "      <th>predictive parity</th>\n",
       "      <th>equality opportunity</th>\n",
       "      <th>predictive equality</th>\n",
       "      <th>average absolute odds</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>prec</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>mcc</th>\n",
       "      <th>selection rate</th>\n",
       "      <th>false positive rate</th>\n",
       "      <th>true positive rate</th>\n",
       "      <th>false negative rate</th>\n",
       "      <th>true negative rate</th>\n",
       "      <th>count</th>\n",
       "      <th>best_trial</th>\n",
       "      <th>fair_metric</th>\n",
       "      <th>model_metric</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.152969</td>\n",
       "      <td>0.067696</td>\n",
       "      <td>0.105239</td>\n",
       "      <td>0.054550</td>\n",
       "      <td>0.079895</td>\n",
       "      <td>0.854686</td>\n",
       "      <td>0.797293</td>\n",
       "      <td>0.528181</td>\n",
       "      <td>0.635418</td>\n",
       "      <td>0.567466</td>\n",
       "      <td>0.158827</td>\n",
       "      <td>0.042348</td>\n",
       "      <td>0.528181</td>\n",
       "      <td>0.471819</td>\n",
       "      <td>0.957652</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155004</td>\n",
       "      <td>0.546969</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187880</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.075670</td>\n",
       "      <td>0.084355</td>\n",
       "      <td>0.080012</td>\n",
       "      <td>0.852741</td>\n",
       "      <td>0.713442</td>\n",
       "      <td>0.644748</td>\n",
       "      <td>0.677358</td>\n",
       "      <td>0.583526</td>\n",
       "      <td>0.216666</td>\n",
       "      <td>0.081667</td>\n",
       "      <td>0.644748</td>\n",
       "      <td>0.355252</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.183918</td>\n",
       "      <td>0.730572</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187071</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.078167</td>\n",
       "      <td>0.083414</td>\n",
       "      <td>0.080791</td>\n",
       "      <td>0.853611</td>\n",
       "      <td>0.718600</td>\n",
       "      <td>0.640051</td>\n",
       "      <td>0.677055</td>\n",
       "      <td>0.584450</td>\n",
       "      <td>0.213544</td>\n",
       "      <td>0.079041</td>\n",
       "      <td>0.640051</td>\n",
       "      <td>0.359949</td>\n",
       "      <td>0.920959</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.178230</td>\n",
       "      <td>0.706446</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033137</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.016510</td>\n",
       "      <td>0.794697</td>\n",
       "      <td>0.986975</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>0.253767</td>\n",
       "      <td>0.335138</td>\n",
       "      <td>0.035369</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>0.854398</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047478</td>\n",
       "      <td>0.248936</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.144394</td>\n",
       "      <td>0.173891</td>\n",
       "      <td>0.337491</td>\n",
       "      <td>0.038278</td>\n",
       "      <td>0.187884</td>\n",
       "      <td>0.829401</td>\n",
       "      <td>0.830965</td>\n",
       "      <td>0.362084</td>\n",
       "      <td>0.504387</td>\n",
       "      <td>0.472977</td>\n",
       "      <td>0.104468</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.362084</td>\n",
       "      <td>0.637916</td>\n",
       "      <td>0.976772</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.110999</td>\n",
       "      <td>0.305293</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.153196</td>\n",
       "      <td>0.067009</td>\n",
       "      <td>0.107707</td>\n",
       "      <td>0.054550</td>\n",
       "      <td>0.081129</td>\n",
       "      <td>0.854532</td>\n",
       "      <td>0.797097</td>\n",
       "      <td>0.527541</td>\n",
       "      <td>0.634892</td>\n",
       "      <td>0.566940</td>\n",
       "      <td>0.158673</td>\n",
       "      <td>0.042348</td>\n",
       "      <td>0.527541</td>\n",
       "      <td>0.472459</td>\n",
       "      <td>0.957652</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.154452</td>\n",
       "      <td>0.545688</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.057531</td>\n",
       "      <td>0.085767</td>\n",
       "      <td>0.058144</td>\n",
       "      <td>0.071956</td>\n",
       "      <td>0.862927</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.577071</td>\n",
       "      <td>0.668728</td>\n",
       "      <td>0.596979</td>\n",
       "      <td>0.174029</td>\n",
       "      <td>0.046927</td>\n",
       "      <td>0.577071</td>\n",
       "      <td>0.422929</td>\n",
       "      <td>0.953073</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.166155</td>\n",
       "      <td>0.589882</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160157</td>\n",
       "      <td>0.061351</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>0.057447</td>\n",
       "      <td>0.076541</td>\n",
       "      <td>0.860675</td>\n",
       "      <td>0.795660</td>\n",
       "      <td>0.563621</td>\n",
       "      <td>0.659835</td>\n",
       "      <td>0.588942</td>\n",
       "      <td>0.169832</td>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.563621</td>\n",
       "      <td>0.436379</td>\n",
       "      <td>0.954353</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.162997</td>\n",
       "      <td>0.574723</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190324</td>\n",
       "      <td>0.020519</td>\n",
       "      <td>0.090863</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.084946</td>\n",
       "      <td>0.864258</td>\n",
       "      <td>0.746124</td>\n",
       "      <td>0.657558</td>\n",
       "      <td>0.699047</td>\n",
       "      <td>0.613898</td>\n",
       "      <td>0.211291</td>\n",
       "      <td>0.070558</td>\n",
       "      <td>0.657558</td>\n",
       "      <td>0.342442</td>\n",
       "      <td>0.929442</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.186306</td>\n",
       "      <td>0.731855</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.158695</td>\n",
       "      <td>0.062478</td>\n",
       "      <td>0.089621</td>\n",
       "      <td>0.057225</td>\n",
       "      <td>0.073423</td>\n",
       "      <td>0.860214</td>\n",
       "      <td>0.795461</td>\n",
       "      <td>0.561272</td>\n",
       "      <td>0.658155</td>\n",
       "      <td>0.587342</td>\n",
       "      <td>0.169166</td>\n",
       "      <td>0.045513</td>\n",
       "      <td>0.561272</td>\n",
       "      <td>0.438728</td>\n",
       "      <td>0.954487</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.162373</td>\n",
       "      <td>0.572801</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165751</td>\n",
       "      <td>0.055439</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.058315</td>\n",
       "      <td>0.079353</td>\n",
       "      <td>0.865281</td>\n",
       "      <td>0.797909</td>\n",
       "      <td>0.586678</td>\n",
       "      <td>0.676181</td>\n",
       "      <td>0.604802</td>\n",
       "      <td>0.176281</td>\n",
       "      <td>0.046859</td>\n",
       "      <td>0.586678</td>\n",
       "      <td>0.413322</td>\n",
       "      <td>0.953141</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.173962</td>\n",
       "      <td>0.609735</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140366</td>\n",
       "      <td>0.167883</td>\n",
       "      <td>0.336126</td>\n",
       "      <td>0.035726</td>\n",
       "      <td>0.185926</td>\n",
       "      <td>0.828019</td>\n",
       "      <td>0.836382</td>\n",
       "      <td>0.351409</td>\n",
       "      <td>0.494889</td>\n",
       "      <td>0.467722</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>0.021679</td>\n",
       "      <td>0.351409</td>\n",
       "      <td>0.648591</td>\n",
       "      <td>0.978321</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.103702</td>\n",
       "      <td>0.287786</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.996255</td>\n",
       "      <td>0.067359</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.033735</td>\n",
       "      <td>0.773814</td>\n",
       "      <td>0.996255</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.107453</td>\n",
       "      <td>0.208578</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.943211</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.014120</td>\n",
       "      <td>0.038856</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025073</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.082046</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.041079</td>\n",
       "      <td>0.776783</td>\n",
       "      <td>0.996923</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>0.230671</td>\n",
       "      <td>0.016635</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>0.930828</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.027102</td>\n",
       "      <td>0.074937</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160699</td>\n",
       "      <td>0.065160</td>\n",
       "      <td>0.093926</td>\n",
       "      <td>0.058173</td>\n",
       "      <td>0.076050</td>\n",
       "      <td>0.860777</td>\n",
       "      <td>0.795072</td>\n",
       "      <td>0.564902</td>\n",
       "      <td>0.660509</td>\n",
       "      <td>0.589387</td>\n",
       "      <td>0.170343</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.564902</td>\n",
       "      <td>0.435098</td>\n",
       "      <td>0.954083</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.163763</td>\n",
       "      <td>0.578139</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189094</td>\n",
       "      <td>0.012641</td>\n",
       "      <td>0.069605</td>\n",
       "      <td>0.078134</td>\n",
       "      <td>0.073870</td>\n",
       "      <td>0.866510</td>\n",
       "      <td>0.745855</td>\n",
       "      <td>0.672289</td>\n",
       "      <td>0.707164</td>\n",
       "      <td>0.622420</td>\n",
       "      <td>0.216103</td>\n",
       "      <td>0.072241</td>\n",
       "      <td>0.672289</td>\n",
       "      <td>0.327711</td>\n",
       "      <td>0.927759</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.183018</td>\n",
       "      <td>0.729505</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023530</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>0.076982</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.038546</td>\n",
       "      <td>0.775759</td>\n",
       "      <td>0.996721</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>0.121868</td>\n",
       "      <td>0.223285</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.064902</td>\n",
       "      <td>0.935098</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.067463</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011727</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>0.038238</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>0.767928</td>\n",
       "      <td>0.993421</td>\n",
       "      <td>0.032237</td>\n",
       "      <td>0.062448</td>\n",
       "      <td>0.156319</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.032237</td>\n",
       "      <td>0.967763</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.018360</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.060704</td>\n",
       "      <td>0.084106</td>\n",
       "      <td>0.072405</td>\n",
       "      <td>0.857194</td>\n",
       "      <td>0.727207</td>\n",
       "      <td>0.647096</td>\n",
       "      <td>0.684817</td>\n",
       "      <td>0.594593</td>\n",
       "      <td>0.213339</td>\n",
       "      <td>0.076550</td>\n",
       "      <td>0.647096</td>\n",
       "      <td>0.352904</td>\n",
       "      <td>0.923450</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182591</td>\n",
       "      <td>0.725236</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166640</td>\n",
       "      <td>0.055990</td>\n",
       "      <td>0.087197</td>\n",
       "      <td>0.061555</td>\n",
       "      <td>0.074376</td>\n",
       "      <td>0.863797</td>\n",
       "      <td>0.788259</td>\n",
       "      <td>0.590521</td>\n",
       "      <td>0.675211</td>\n",
       "      <td>0.601143</td>\n",
       "      <td>0.179608</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.590521</td>\n",
       "      <td>0.409479</td>\n",
       "      <td>0.949976</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.168987</td>\n",
       "      <td>0.601836</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.996269</td>\n",
       "      <td>0.067612</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.773865</td>\n",
       "      <td>0.996269</td>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.107835</td>\n",
       "      <td>0.208978</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.942997</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.025619</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.169042</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>0.079669</td>\n",
       "      <td>0.064297</td>\n",
       "      <td>0.071983</td>\n",
       "      <td>0.864923</td>\n",
       "      <td>0.787623</td>\n",
       "      <td>0.597780</td>\n",
       "      <td>0.679694</td>\n",
       "      <td>0.605238</td>\n",
       "      <td>0.181962</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>0.597780</td>\n",
       "      <td>0.402220</td>\n",
       "      <td>0.949169</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.170415</td>\n",
       "      <td>0.607174</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170856</td>\n",
       "      <td>0.041015</td>\n",
       "      <td>0.090781</td>\n",
       "      <td>0.061618</td>\n",
       "      <td>0.076199</td>\n",
       "      <td>0.867431</td>\n",
       "      <td>0.788271</td>\n",
       "      <td>0.611230</td>\n",
       "      <td>0.688552</td>\n",
       "      <td>0.613962</td>\n",
       "      <td>0.185904</td>\n",
       "      <td>0.051774</td>\n",
       "      <td>0.611230</td>\n",
       "      <td>0.388770</td>\n",
       "      <td>0.948226</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.635782</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.030949</td>\n",
       "      <td>0.772688</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.052092</td>\n",
       "      <td>0.099006</td>\n",
       "      <td>0.199598</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.052092</td>\n",
       "      <td>0.947908</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>0.030954</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.996894</td>\n",
       "      <td>0.081286</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.040699</td>\n",
       "      <td>0.776629</td>\n",
       "      <td>0.996894</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.128246</td>\n",
       "      <td>0.229577</td>\n",
       "      <td>0.016482</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.068531</td>\n",
       "      <td>0.931469</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.061275</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050323</td>\n",
       "      <td>0.032389</td>\n",
       "      <td>0.120340</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.798024</td>\n",
       "      <td>0.969466</td>\n",
       "      <td>0.162681</td>\n",
       "      <td>0.278611</td>\n",
       "      <td>0.349941</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.162681</td>\n",
       "      <td>0.837319</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.045627</td>\n",
       "      <td>0.150944</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174088</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>0.081262</td>\n",
       "      <td>0.065535</td>\n",
       "      <td>0.073398</td>\n",
       "      <td>0.866510</td>\n",
       "      <td>0.775624</td>\n",
       "      <td>0.623612</td>\n",
       "      <td>0.691361</td>\n",
       "      <td>0.613360</td>\n",
       "      <td>0.192762</td>\n",
       "      <td>0.056891</td>\n",
       "      <td>0.623612</td>\n",
       "      <td>0.376388</td>\n",
       "      <td>0.943109</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.177775</td>\n",
       "      <td>0.644535</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157906</td>\n",
       "      <td>0.054524</td>\n",
       "      <td>0.098574</td>\n",
       "      <td>0.055612</td>\n",
       "      <td>0.077093</td>\n",
       "      <td>0.859702</td>\n",
       "      <td>0.797004</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.655437</td>\n",
       "      <td>0.585323</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.443424</td>\n",
       "      <td>0.955295</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.161110</td>\n",
       "      <td>0.568319</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042751</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.134752</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.067653</td>\n",
       "      <td>0.788862</td>\n",
       "      <td>0.991213</td>\n",
       "      <td>0.120410</td>\n",
       "      <td>0.214734</td>\n",
       "      <td>0.304856</td>\n",
       "      <td>0.029124</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.120410</td>\n",
       "      <td>0.879590</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032788</td>\n",
       "      <td>0.990588</td>\n",
       "      <td>0.106609</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.053527</td>\n",
       "      <td>0.781594</td>\n",
       "      <td>0.990588</td>\n",
       "      <td>0.089880</td>\n",
       "      <td>0.164807</td>\n",
       "      <td>0.262259</td>\n",
       "      <td>0.021754</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.089880</td>\n",
       "      <td>0.910120</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.029895</td>\n",
       "      <td>0.082408</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030114</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>0.786968</td>\n",
       "      <td>0.992453</td>\n",
       "      <td>0.112297</td>\n",
       "      <td>0.201764</td>\n",
       "      <td>0.294406</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.112297</td>\n",
       "      <td>0.887703</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>0.094149</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.166487</td>\n",
       "      <td>0.057330</td>\n",
       "      <td>0.085077</td>\n",
       "      <td>0.061888</td>\n",
       "      <td>0.073482</td>\n",
       "      <td>0.863541</td>\n",
       "      <td>0.787464</td>\n",
       "      <td>0.590094</td>\n",
       "      <td>0.674640</td>\n",
       "      <td>0.600377</td>\n",
       "      <td>0.179659</td>\n",
       "      <td>0.050226</td>\n",
       "      <td>0.590094</td>\n",
       "      <td>0.409906</td>\n",
       "      <td>0.949774</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.168819</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020907</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.034241</td>\n",
       "      <td>0.774019</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.057643</td>\n",
       "      <td>0.108981</td>\n",
       "      <td>0.210172</td>\n",
       "      <td>0.013871</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.057643</td>\n",
       "      <td>0.942357</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.040137</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   demographic parity  predictive parity  equality opportunity   \n",
       "0            0.152969           0.067696              0.105239  \\\n",
       "0            0.187880           0.008934              0.075670   \n",
       "0            0.187071           0.001063              0.078167   \n",
       "0            0.033137           0.001903              0.032304   \n",
       "0            0.144394           0.173891              0.337491   \n",
       "0            0.153196           0.067009              0.107707   \n",
       "0            0.161669           0.057531              0.085767   \n",
       "0            0.160157           0.061351              0.095634   \n",
       "0            0.190324           0.020519              0.090863   \n",
       "0            0.158695           0.062478              0.089621   \n",
       "0            0.165751           0.055439              0.100390   \n",
       "0            0.140366           0.167883              0.336126   \n",
       "0            0.020599           0.996255              0.067359   \n",
       "0            0.025073           0.996923              0.082046   \n",
       "0            0.160699           0.065160              0.093926   \n",
       "0            0.189094           0.012641              0.069605   \n",
       "0            0.023530           0.996721              0.076982   \n",
       "0            0.011727           0.993421              0.038238   \n",
       "0            0.186992           0.022068              0.060704   \n",
       "0            0.166640           0.055990              0.087197   \n",
       "0            0.020676           0.996269              0.067612   \n",
       "0            0.169042           0.071249              0.079669   \n",
       "0            0.170856           0.041015              0.090781   \n",
       "0            0.018901           0.995918              0.061788   \n",
       "0            0.024842           0.996894              0.081286   \n",
       "0            0.050323           0.032389              0.120340   \n",
       "0            0.174088           0.031818              0.081262   \n",
       "0            0.157906           0.054524              0.098574   \n",
       "0            0.042751           0.008865              0.134752   \n",
       "0            0.032788           0.990588              0.106609   \n",
       "0            0.030114           0.008282              0.057351   \n",
       "0            0.166487           0.057330              0.085077   \n",
       "0            0.020907           0.996310              0.068372   \n",
       "\n",
       "   predictive equality  average absolute odds  accuracy      prec    recall   \n",
       "0             0.054550               0.079895  0.854686  0.797293  0.528181  \\\n",
       "0             0.084355               0.080012  0.852741  0.713442  0.644748   \n",
       "0             0.083414               0.080791  0.853611  0.718600  0.640051   \n",
       "0             0.000716               0.016510  0.794697  0.986975  0.145602   \n",
       "0             0.038278               0.187884  0.829401  0.830965  0.362084   \n",
       "0             0.054550               0.081129  0.854532  0.797097  0.527541   \n",
       "0             0.058144               0.071956  0.862927  0.795000  0.577071   \n",
       "0             0.057447               0.076541  0.860675  0.795660  0.563621   \n",
       "0             0.079028               0.084946  0.864258  0.746124  0.657558   \n",
       "0             0.057225               0.073423  0.860214  0.795461  0.561272   \n",
       "0             0.058315               0.079353  0.865281  0.797909  0.586678   \n",
       "0             0.035726               0.185926  0.828019  0.836382  0.351409   \n",
       "0             0.000111               0.033735  0.773814  0.996255  0.056789   \n",
       "0             0.000111               0.041079  0.776783  0.996923  0.069172   \n",
       "0             0.058173               0.076050  0.860777  0.795072  0.564902   \n",
       "0             0.078134               0.073870  0.866510  0.745855  0.672289   \n",
       "0             0.000111               0.038546  0.775759  0.996721  0.064902   \n",
       "0             0.000111               0.019174  0.767928  0.993421  0.032237   \n",
       "0             0.084106               0.072405  0.857194  0.727207  0.647096   \n",
       "0             0.061555               0.074376  0.863797  0.788259  0.590521   \n",
       "0             0.000111               0.033862  0.773865  0.996269  0.057003   \n",
       "0             0.064297               0.071983  0.864923  0.787623  0.597780   \n",
       "0             0.061618               0.076199  0.867431  0.788271  0.611230   \n",
       "0             0.000111               0.030949  0.772688  0.995918  0.052092   \n",
       "0             0.000111               0.040699  0.776629  0.996894  0.068531   \n",
       "0             0.002663               0.061502  0.798024  0.969466  0.162681   \n",
       "0             0.065535               0.073398  0.866510  0.775624  0.623612   \n",
       "0             0.055612               0.077093  0.859702  0.797004  0.556576   \n",
       "0             0.000555               0.067653  0.788862  0.991213  0.120410   \n",
       "0             0.000444               0.053527  0.781594  0.990588  0.089880   \n",
       "0             0.000444               0.028897  0.786968  0.992453  0.112297   \n",
       "0             0.061888               0.073482  0.863541  0.787464  0.590094   \n",
       "0             0.000111               0.034241  0.774019  0.996310  0.057643   \n",
       "\n",
       "   f1 score       mcc  selection rate  false positive rate   \n",
       "0  0.635418  0.567466        0.158827             0.042348  \\\n",
       "0  0.677358  0.583526        0.216666             0.081667   \n",
       "0  0.677055  0.584450        0.213544             0.079041   \n",
       "0  0.253767  0.335138        0.035369             0.000606   \n",
       "0  0.504387  0.472977        0.104468             0.023228   \n",
       "0  0.634892  0.566940        0.158673             0.042348   \n",
       "0  0.668728  0.596979        0.174029             0.046927   \n",
       "0  0.659835  0.588942        0.169832             0.045647   \n",
       "0  0.699047  0.613898        0.211291             0.070558   \n",
       "0  0.658155  0.587342        0.169166             0.045513   \n",
       "0  0.676181  0.604802        0.176281             0.046859   \n",
       "0  0.494889  0.467722        0.100732             0.021679   \n",
       "0  0.107453  0.208578        0.013666             0.000067   \n",
       "0  0.129367  0.230671        0.016635             0.000067   \n",
       "0  0.660509  0.589387        0.170343             0.045917   \n",
       "0  0.707164  0.622420        0.216103             0.072241   \n",
       "0  0.121868  0.223285        0.015611             0.000067   \n",
       "0  0.062448  0.156319        0.007780             0.000067   \n",
       "0  0.684817  0.594593        0.213339             0.076550   \n",
       "0  0.675211  0.601143        0.179608             0.050024   \n",
       "0  0.107835  0.208978        0.013718             0.000067   \n",
       "0  0.679694  0.605238        0.181962             0.050831   \n",
       "0  0.688552  0.613962        0.185904             0.051774   \n",
       "0  0.099006  0.199598        0.012540             0.000067   \n",
       "0  0.128246  0.229577        0.016482             0.000067   \n",
       "0  0.278611  0.349941        0.040231             0.001616   \n",
       "0  0.691361  0.613360        0.192762             0.056891   \n",
       "0  0.655437  0.585323        0.167426             0.044705   \n",
       "0  0.214734  0.304856        0.029124             0.000337   \n",
       "0  0.164807  0.262259        0.021754             0.000269   \n",
       "0  0.201764  0.294406        0.027128             0.000269   \n",
       "0  0.674640  0.600377        0.179659             0.050226   \n",
       "0  0.108981  0.210172        0.013871             0.000067   \n",
       "\n",
       "   true positive rate  false negative rate  true negative rate    count   \n",
       "0            0.528181             0.471819            0.957652  19537.0  \\\n",
       "0            0.644748             0.355252            0.918333  19537.0   \n",
       "0            0.640051             0.359949            0.920959  19537.0   \n",
       "0            0.145602             0.854398            0.999394  19537.0   \n",
       "0            0.362084             0.637916            0.976772  19537.0   \n",
       "0            0.527541             0.472459            0.957652  19537.0   \n",
       "0            0.577071             0.422929            0.953073  19537.0   \n",
       "0            0.563621             0.436379            0.954353  19537.0   \n",
       "0            0.657558             0.342442            0.929442  19537.0   \n",
       "0            0.561272             0.438728            0.954487  19537.0   \n",
       "0            0.586678             0.413322            0.953141  19537.0   \n",
       "0            0.351409             0.648591            0.978321  19537.0   \n",
       "0            0.056789             0.943211            0.999933  19537.0   \n",
       "0            0.069172             0.930828            0.999933  19537.0   \n",
       "0            0.564902             0.435098            0.954083  19537.0   \n",
       "0            0.672289             0.327711            0.927759  19537.0   \n",
       "0            0.064902             0.935098            0.999933  19537.0   \n",
       "0            0.032237             0.967763            0.999933  19537.0   \n",
       "0            0.647096             0.352904            0.923450  19537.0   \n",
       "0            0.590521             0.409479            0.949976  19537.0   \n",
       "0            0.057003             0.942997            0.999933  19537.0   \n",
       "0            0.597780             0.402220            0.949169  19537.0   \n",
       "0            0.611230             0.388770            0.948226  19537.0   \n",
       "0            0.052092             0.947908            0.999933  19537.0   \n",
       "0            0.068531             0.931469            0.999933  19537.0   \n",
       "0            0.162681             0.837319            0.998384  19537.0   \n",
       "0            0.623612             0.376388            0.943109  19537.0   \n",
       "0            0.556576             0.443424            0.955295  19537.0   \n",
       "0            0.120410             0.879590            0.999663  19537.0   \n",
       "0            0.089880             0.910120            0.999731  19537.0   \n",
       "0            0.112297             0.887703            0.999731  19537.0   \n",
       "0            0.590094             0.409906            0.949774  19537.0   \n",
       "0            0.057643             0.942357            0.999933  19537.0   \n",
       "\n",
       "   best_trial  fair_metric  model_metric model_name  \n",
       "0           1     0.155004      0.546969        GBM  \n",
       "0           2     0.183918      0.730572       LGBM  \n",
       "0           3     0.178230      0.706446       LGBM  \n",
       "0           4     0.047478      0.248936       LGBM  \n",
       "0           5     0.110999      0.305293         RF  \n",
       "0           6     0.154452      0.545688        GBM  \n",
       "0           7     0.166155      0.589882        GBM  \n",
       "0           8     0.162997      0.574723        GBM  \n",
       "0           9     0.186306      0.731855       LGBM  \n",
       "0          10     0.162373      0.572801        GBM  \n",
       "0          11     0.173962      0.609735        GBM  \n",
       "0          12     0.103702      0.287786         RF  \n",
       "0          13     0.014120      0.038856         RF  \n",
       "0          14     0.027102      0.074937         RF  \n",
       "0          15     0.163763      0.578139        GBM  \n",
       "0          16     0.183018      0.729505       LGBM  \n",
       "0          17     0.024400      0.067463         RF  \n",
       "0          18     0.006659      0.018360         RF  \n",
       "0          19     0.182591      0.725236       LGBM  \n",
       "0          20     0.168987      0.601836        GBM  \n",
       "0          21     0.009271      0.025619         RF  \n",
       "0          22     0.170415      0.607174        GBM  \n",
       "0          23     0.177238      0.635782        GBM  \n",
       "0          24     0.011177      0.030954         RF  \n",
       "0          25     0.022147      0.061275         RF  \n",
       "0          26     0.045627      0.150944         RF  \n",
       "0          27     0.177775      0.644535        GBM  \n",
       "0          28     0.161110      0.568319        GBM  \n",
       "0          29     0.031987      0.088600         RF  \n",
       "0          30     0.029895      0.082408         RF  \n",
       "0          31     0.033215      0.094149         RF  \n",
       "0          32     0.168819      0.601196        GBM  \n",
       "0          33     0.014489      0.040137         RF  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['overall'].rename(columns = {'precision':'prec'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex                            NaN\n",
       "accuracy                  0.092950\n",
       "precision                 0.014980\n",
       "recall                    0.139323\n",
       "f1 score                  0.076544\n",
       "mcc                       0.008755\n",
       "selection rate            0.202506\n",
       "false positive rate       0.080465\n",
       "true positive rate        0.139323\n",
       "false negative rate       0.139323\n",
       "true negative rate        0.080465\n",
       "count                  6461.000000\n",
       "model                          NaN\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = metrics['default_bygroup'][n_model].apply(pd.to_numeric, errors='coerce').diff().abs().iloc[-1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_groups_metrics(n, results_dict, model_mapping):\n",
    "    model = metrics['overall'][metrics['overall'].index == 5].model_name\n",
    "    n_model = metrics['default_bygroup']['model'].isin(model)\n",
    "    #models = list(map(model_mapping.get,results_dict['models_sim_u'][0]))\n",
    "    #n_model = models.index(results_dict['models_sim'][0][n])\n",
    "\n",
    "    #df_groups_u = metrics['']\n",
    "    df_groups_u = results_dict['metrics_sim_u'][0][n_model].by_group.T\n",
    "    d = results_dict['metrics_sim_u'][0][n_model].difference()\n",
    "    d.name = 'Difference'\n",
    "    df_groups_u = pd.concat([df_groups_u,d], axis = 1).T\n",
    "    df_groups_u.columns = df_groups_u.columns + ' u'\n",
    "\n",
    "    df_groups_m = results_dict['metrics_sim'][0][n].by_group.T\n",
    "    d = results_dict['metrics_sim'][0][n].difference()\n",
    "    d.name = 'Difference'\n",
    "    df_groups_m = pd.concat([df_groups_m,d], axis = 1).T\n",
    "    df_groups = pd.concat([df_groups_u,df_groups_m],axis = 1).reset_index()\n",
    "    return df_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../notebooks/metrics.json'\n",
    "with open(file_name, 'rb') as f:\n",
    "    metrics_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demographic parity</th>\n",
       "      <th>predictive parity</th>\n",
       "      <th>equality opportunity</th>\n",
       "      <th>predictive equality</th>\n",
       "      <th>average absolute odds</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>mcc</th>\n",
       "      <th>selection rate</th>\n",
       "      <th>false positive rate</th>\n",
       "      <th>true positive rate</th>\n",
       "      <th>false negative rate</th>\n",
       "      <th>true negative rate</th>\n",
       "      <th>count</th>\n",
       "      <th>best_trial</th>\n",
       "      <th>fair_metric</th>\n",
       "      <th>model_metric</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.767723</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>0.154197</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.968617</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052761</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.079727</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.039919</td>\n",
       "      <td>0.776322</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.227375</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.932750</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.129615</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.091722</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.046082</td>\n",
       "      <td>0.791677</td>\n",
       "      <td>0.993569</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.232944</td>\n",
       "      <td>0.320185</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.868061</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.211406</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.784153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.278665</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.900299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.298484</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.790039</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.124893</td>\n",
       "      <td>0.221927</td>\n",
       "      <td>0.311580</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.124893</td>\n",
       "      <td>0.875107</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.392168</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036005</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.043393</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.796796</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.266445</td>\n",
       "      <td>0.345902</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.846072</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.398623</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>0.825869</td>\n",
       "      <td>0.969941</td>\n",
       "      <td>0.282451</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.468568</td>\n",
       "      <td>0.069816</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.282451</td>\n",
       "      <td>0.717549</td>\n",
       "      <td>0.997240</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.417735</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.054103</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.822337</td>\n",
       "      <td>0.972720</td>\n",
       "      <td>0.266439</td>\n",
       "      <td>0.418301</td>\n",
       "      <td>0.455159</td>\n",
       "      <td>0.065670</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.266439</td>\n",
       "      <td>0.733561</td>\n",
       "      <td>0.997644</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.427776</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.068367</td>\n",
       "      <td>0.026821</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.831448</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.473709</td>\n",
       "      <td>0.486843</td>\n",
       "      <td>0.080514</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.683604</td>\n",
       "      <td>0.993873</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.471810</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.042162</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.812510</td>\n",
       "      <td>0.976657</td>\n",
       "      <td>0.223313</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>0.415684</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.223313</td>\n",
       "      <td>0.776687</td>\n",
       "      <td>0.998317</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.472184</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.027780</td>\n",
       "      <td>0.052258</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.802631</td>\n",
       "      <td>0.923313</td>\n",
       "      <td>0.192784</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.367547</td>\n",
       "      <td>0.050059</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.192784</td>\n",
       "      <td>0.807216</td>\n",
       "      <td>0.994951</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.493216</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.088155</td>\n",
       "      <td>0.049605</td>\n",
       "      <td>0.052525</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.839945</td>\n",
       "      <td>0.912560</td>\n",
       "      <td>0.367635</td>\n",
       "      <td>0.524121</td>\n",
       "      <td>0.515286</td>\n",
       "      <td>0.096586</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>0.367635</td>\n",
       "      <td>0.632365</td>\n",
       "      <td>0.988891</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.514874</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.091633</td>\n",
       "      <td>0.026325</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>0.839689</td>\n",
       "      <td>0.890735</td>\n",
       "      <td>0.377669</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.512923</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.377669</td>\n",
       "      <td>0.622331</td>\n",
       "      <td>0.985390</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.539128</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.108260</td>\n",
       "      <td>0.043261</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>0.054603</td>\n",
       "      <td>0.846343</td>\n",
       "      <td>0.875782</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.566310</td>\n",
       "      <td>0.535847</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.581554</td>\n",
       "      <td>0.981283</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.589778</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.120669</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.092362</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>0.061262</td>\n",
       "      <td>0.852024</td>\n",
       "      <td>0.861930</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.976974</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.618610</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.141611</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.080007</td>\n",
       "      <td>0.854225</td>\n",
       "      <td>0.827156</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.619760</td>\n",
       "      <td>0.563461</td>\n",
       "      <td>0.143625</td>\n",
       "      <td>0.032653</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.504483</td>\n",
       "      <td>0.967347</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.039348</td>\n",
       "      <td>0.635345</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.051462</td>\n",
       "      <td>0.133908</td>\n",
       "      <td>0.056247</td>\n",
       "      <td>0.095078</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>0.799819</td>\n",
       "      <td>0.567250</td>\n",
       "      <td>0.663752</td>\n",
       "      <td>0.593780</td>\n",
       "      <td>0.170036</td>\n",
       "      <td>0.044772</td>\n",
       "      <td>0.567250</td>\n",
       "      <td>0.432750</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.177417</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.067336</td>\n",
       "      <td>0.104360</td>\n",
       "      <td>0.858320</td>\n",
       "      <td>0.763476</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.667308</td>\n",
       "      <td>0.586606</td>\n",
       "      <td>0.186108</td>\n",
       "      <td>0.057901</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>0.942099</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.696462</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.178495</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.142298</td>\n",
       "      <td>0.068109</td>\n",
       "      <td>0.105203</td>\n",
       "      <td>0.858474</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>0.594791</td>\n",
       "      <td>0.668346</td>\n",
       "      <td>0.587374</td>\n",
       "      <td>0.186979</td>\n",
       "      <td>0.058372</td>\n",
       "      <td>0.594791</td>\n",
       "      <td>0.405209</td>\n",
       "      <td>0.941628</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.042791</td>\n",
       "      <td>0.702810</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.184179</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>0.138032</td>\n",
       "      <td>0.071359</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>0.864155</td>\n",
       "      <td>0.774324</td>\n",
       "      <td>0.611657</td>\n",
       "      <td>0.683445</td>\n",
       "      <td>0.605222</td>\n",
       "      <td>0.189384</td>\n",
       "      <td>0.056218</td>\n",
       "      <td>0.611657</td>\n",
       "      <td>0.388343</td>\n",
       "      <td>0.943782</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>0.704688</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.196430</td>\n",
       "      <td>0.067135</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>0.082776</td>\n",
       "      <td>0.114262</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>0.752932</td>\n",
       "      <td>0.630444</td>\n",
       "      <td>0.686265</td>\n",
       "      <td>0.602416</td>\n",
       "      <td>0.200747</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.630444</td>\n",
       "      <td>0.369556</td>\n",
       "      <td>0.934761</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>0.711351</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.203385</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.118353</td>\n",
       "      <td>0.086242</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>0.865179</td>\n",
       "      <td>0.734661</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>0.709015</td>\n",
       "      <td>0.622060</td>\n",
       "      <td>0.223576</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>0.314902</td>\n",
       "      <td>0.921969</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.764921</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    demographic parity  predictive parity  equality opportunity   \n",
       "0             0.011385           0.993243              0.037206  \\\n",
       "1             0.024310           0.996835              0.079727   \n",
       "2             0.038655           0.006873              0.091722   \n",
       "3             0.021673           0.000000              0.017922   \n",
       "4             0.028683           0.005814              0.031614   \n",
       "5             0.036005           0.002134              0.043393   \n",
       "6             0.057346           0.006960              0.008146   \n",
       "7             0.054103           0.001775              0.010189   \n",
       "8             0.068367           0.026821              0.017659   \n",
       "9             0.042162           0.006250              0.010208   \n",
       "10            0.048800           0.027780              0.052258   \n",
       "11            0.088155           0.049605              0.052525   \n",
       "12            0.091633           0.026325              0.054717   \n",
       "13            0.108260           0.043261              0.085268   \n",
       "14            0.120669           0.056145              0.092362   \n",
       "15            0.141611           0.086868              0.115178   \n",
       "16            0.164985           0.051462              0.133908   \n",
       "17            0.177417           0.018572              0.141384   \n",
       "18            0.178495           0.020057              0.142298   \n",
       "19            0.184179           0.063885              0.138032   \n",
       "20            0.196430           0.067135              0.145749   \n",
       "21            0.203385           0.010185              0.118353   \n",
       "\n",
       "    predictive equality  average absolute odds  accuracy  precision    recall   \n",
       "0              0.000111               0.018658  0.767723   0.993243  0.031383  \\\n",
       "1              0.000111               0.039919  0.776322   0.996835  0.067250   \n",
       "2              0.000442               0.046082  0.791677   0.993569  0.131939   \n",
       "3              0.000000               0.008961  0.784153   1.000000  0.099701   \n",
       "4              0.000332               0.015973  0.790039   0.994898  0.124893   \n",
       "5              0.000491               0.021942  0.796796   0.990385  0.153928   \n",
       "6              0.003117               0.005632  0.825869   0.969941  0.282451   \n",
       "7              0.002454               0.006322  0.822337   0.972720  0.266439   \n",
       "8              0.007795               0.012727  0.831448   0.942149  0.316396   \n",
       "9              0.001349               0.005778  0.812510   0.976657  0.223313   \n",
       "10             0.006592               0.029425  0.802631   0.923313  0.192784   \n",
       "11             0.015125               0.033825  0.839945   0.912560  0.367635   \n",
       "12             0.017479               0.036098  0.839689   0.890735  0.377669   \n",
       "13             0.023938               0.054603  0.846343   0.875782  0.418446   \n",
       "14             0.030163               0.061262  0.852024   0.861930  0.455807   \n",
       "15             0.044837               0.080007  0.854225   0.827156  0.495517   \n",
       "16             0.056247               0.095078  0.862210   0.799819  0.567250   \n",
       "17             0.067336               0.104360  0.858320   0.763476  0.592656   \n",
       "18             0.068109               0.105203  0.858474   0.762661  0.594791   \n",
       "19             0.071359               0.104696  0.864155   0.774324  0.611657   \n",
       "20             0.082776               0.114262  0.861801   0.752932  0.630444   \n",
       "21             0.086242               0.102298  0.865179   0.734661  0.685098   \n",
       "\n",
       "    f1 score       mcc  selection rate  false positive rate   \n",
       "0   0.060844  0.154197        0.007575             0.000067  \\\n",
       "1   0.126000  0.227375        0.016174             0.000067   \n",
       "2   0.232944  0.320185        0.031837             0.000269   \n",
       "3   0.181324  0.278665        0.023903             0.000000   \n",
       "4   0.221927  0.311580        0.030097             0.000202   \n",
       "5   0.266445  0.345902        0.037263             0.000471   \n",
       "6   0.437500  0.468568        0.069816             0.002760   \n",
       "7   0.418301  0.455159        0.065670             0.002356   \n",
       "8   0.473709  0.486843        0.080514             0.006127   \n",
       "9   0.363510  0.415684        0.054819             0.001683   \n",
       "10  0.318969  0.367547        0.050059             0.005049   \n",
       "11  0.524121  0.515286        0.096586             0.011109   \n",
       "12  0.530435  0.512923        0.101653             0.014610   \n",
       "13  0.566310  0.535847        0.114552             0.018717   \n",
       "14  0.596285  0.555305        0.126785             0.023026   \n",
       "15  0.619760  0.563461        0.143625             0.032653   \n",
       "16  0.663752  0.593780        0.170036             0.044772   \n",
       "17  0.667308  0.586606        0.186108             0.057901   \n",
       "18  0.668346  0.587374        0.186979             0.058372   \n",
       "19  0.683445  0.605222        0.189384             0.056218   \n",
       "20  0.686265  0.602416        0.200747             0.065239   \n",
       "21  0.709015  0.622060        0.223576             0.078031   \n",
       "\n",
       "    true positive rate  false negative rate  true negative rate    count   \n",
       "0             0.031383             0.968617            0.999933  19537.0  \\\n",
       "1             0.067250             0.932750            0.999933  19537.0   \n",
       "2             0.131939             0.868061            0.999731  19537.0   \n",
       "3             0.099701             0.900299            1.000000  19537.0   \n",
       "4             0.124893             0.875107            0.999798  19537.0   \n",
       "5             0.153928             0.846072            0.999529  19537.0   \n",
       "6             0.282451             0.717549            0.997240  19537.0   \n",
       "7             0.266439             0.733561            0.997644  19537.0   \n",
       "8             0.316396             0.683604            0.993873  19537.0   \n",
       "9             0.223313             0.776687            0.998317  19537.0   \n",
       "10            0.192784             0.807216            0.994951  19537.0   \n",
       "11            0.367635             0.632365            0.988891  19537.0   \n",
       "12            0.377669             0.622331            0.985390  19537.0   \n",
       "13            0.418446             0.581554            0.981283  19537.0   \n",
       "14            0.455807             0.544193            0.976974  19537.0   \n",
       "15            0.495517             0.504483            0.967347  19537.0   \n",
       "16            0.567250             0.432750            0.955228  19537.0   \n",
       "17            0.592656             0.407344            0.942099  19537.0   \n",
       "18            0.594791             0.405209            0.941628  19537.0   \n",
       "19            0.611657             0.388343            0.943782  19537.0   \n",
       "20            0.630444             0.369556            0.934761  19537.0   \n",
       "21            0.685098             0.314902            0.921969  19537.0   \n",
       "\n",
       "    best_trial  fair_metric  model_metric model_name  \n",
       "0            2     0.000000      0.052761         RF  \n",
       "1            1     0.000111      0.129615         RF  \n",
       "2            3     0.000219      0.211406         RF  \n",
       "3           17     0.000328      0.298484       LGBM  \n",
       "4            4     0.000726      0.392168       LGBM  \n",
       "5            5     0.000812      0.398623       LGBM  \n",
       "6           19     0.000948      0.417735       LGBM  \n",
       "7           20     0.001751      0.427776       LGBM  \n",
       "8           18     0.003929      0.471810       LGBM  \n",
       "9            7     0.004248      0.472184       LGBM  \n",
       "10           6     0.006572      0.493216       LGBM  \n",
       "11          12     0.006675      0.514874       LGBM  \n",
       "12          14     0.011271      0.539128       LGBM  \n",
       "13          15     0.016921      0.589778       LGBM  \n",
       "14           9     0.021963      0.618610       LGBM  \n",
       "15           8     0.039348      0.635345       LGBM  \n",
       "16          13     0.039683      0.667200       LGBM  \n",
       "17          21     0.041186      0.696462       LGBM  \n",
       "18          22     0.042791      0.702810       LGBM  \n",
       "19          10     0.049580      0.704688       LGBM  \n",
       "20          11     0.052571      0.711351       LGBM  \n",
       "21          16     0.055012      0.764921       LGBM  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['overall']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demographic parity</th>\n",
       "      <th>predictive parity</th>\n",
       "      <th>equality opportunity</th>\n",
       "      <th>predictive equality</th>\n",
       "      <th>average absolute odds</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>mcc</th>\n",
       "      <th>selection rate</th>\n",
       "      <th>false positive rate</th>\n",
       "      <th>true positive rate</th>\n",
       "      <th>false negative rate</th>\n",
       "      <th>true negative rate</th>\n",
       "      <th>count</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.204770</td>\n",
       "      <td>0.056846</td>\n",
       "      <td>0.147274</td>\n",
       "      <td>0.086014</td>\n",
       "      <td>0.116644</td>\n",
       "      <td>0.866561</td>\n",
       "      <td>0.751149</td>\n",
       "      <td>0.663108</td>\n",
       "      <td>0.704388</td>\n",
       "      <td>0.620656</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.069279</td>\n",
       "      <td>0.663108</td>\n",
       "      <td>0.336892</td>\n",
       "      <td>0.930721</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.202506</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.139323</td>\n",
       "      <td>0.080465</td>\n",
       "      <td>0.109894</td>\n",
       "      <td>0.869939</td>\n",
       "      <td>0.752177</td>\n",
       "      <td>0.682323</td>\n",
       "      <td>0.715549</td>\n",
       "      <td>0.632765</td>\n",
       "      <td>0.217485</td>\n",
       "      <td>0.070895</td>\n",
       "      <td>0.682323</td>\n",
       "      <td>0.317677</td>\n",
       "      <td>0.929105</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   demographic parity  predictive parity  equality opportunity   \n",
       "0            0.204770           0.056846              0.147274  \\\n",
       "0            0.202506           0.014980              0.139323   \n",
       "\n",
       "   predictive equality  average absolute odds  accuracy  precision    recall   \n",
       "0             0.086014               0.116644  0.866561   0.751149  0.663108  \\\n",
       "0             0.080465               0.109894  0.869939   0.752177  0.682323   \n",
       "\n",
       "   f1 score       mcc  selection rate  false positive rate   \n",
       "0  0.704388  0.620656        0.211650             0.069279  \\\n",
       "0  0.715549  0.632765        0.217485             0.070895   \n",
       "\n",
       "   true positive rate  false negative rate  true negative rate    count model  \n",
       "0            0.663108             0.336892            0.930721  19537.0    RF  \n",
       "0            0.682323             0.317677            0.929105  19537.0  LGBM  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['default_overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'LGBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3      True\n",
       "4      True\n",
       "5      True\n",
       "6      True\n",
       "7      True\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11     True\n",
       "12     True\n",
       "13     True\n",
       "14     True\n",
       "15     True\n",
       "16     True\n",
       "17     True\n",
       "18     True\n",
       "19     True\n",
       "20     True\n",
       "21     True\n",
       "22    False\n",
       "23    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_overall.model_name == model) & (~df_overall.best_trial.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_default_overall = metrics['default_overall'].rename(columns = {'model':'model_name'})\n",
    "df_optimized_overall = metrics['overall']\n",
    "df_overall = pd.concat([df_optimized_overall,df_default_overall]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demographic parity</th>\n",
       "      <th>predictive parity</th>\n",
       "      <th>equality opportunity</th>\n",
       "      <th>predictive equality</th>\n",
       "      <th>average absolute odds</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>mcc</th>\n",
       "      <th>selection rate</th>\n",
       "      <th>false positive rate</th>\n",
       "      <th>true positive rate</th>\n",
       "      <th>false negative rate</th>\n",
       "      <th>true negative rate</th>\n",
       "      <th>count</th>\n",
       "      <th>best_trial</th>\n",
       "      <th>fair_metric</th>\n",
       "      <th>model_metric</th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.767723</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>0.154197</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.968617</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052761</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.079727</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.039919</td>\n",
       "      <td>0.776322</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.227375</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.067250</td>\n",
       "      <td>0.932750</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.129615</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.091722</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.046082</td>\n",
       "      <td>0.791677</td>\n",
       "      <td>0.993569</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.232944</td>\n",
       "      <td>0.320185</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.868061</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.211406</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.784153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.181324</td>\n",
       "      <td>0.278665</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099701</td>\n",
       "      <td>0.900299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.298484</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028683</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.031614</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.015973</td>\n",
       "      <td>0.790039</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.124893</td>\n",
       "      <td>0.221927</td>\n",
       "      <td>0.311580</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.124893</td>\n",
       "      <td>0.875107</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.392168</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036005</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.043393</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.796796</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.266445</td>\n",
       "      <td>0.345902</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.846072</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.398623</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>0.825869</td>\n",
       "      <td>0.969941</td>\n",
       "      <td>0.282451</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.468568</td>\n",
       "      <td>0.069816</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.282451</td>\n",
       "      <td>0.717549</td>\n",
       "      <td>0.997240</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.417735</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.054103</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.822337</td>\n",
       "      <td>0.972720</td>\n",
       "      <td>0.266439</td>\n",
       "      <td>0.418301</td>\n",
       "      <td>0.455159</td>\n",
       "      <td>0.065670</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.266439</td>\n",
       "      <td>0.733561</td>\n",
       "      <td>0.997644</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.427776</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.068367</td>\n",
       "      <td>0.026821</td>\n",
       "      <td>0.017659</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.831448</td>\n",
       "      <td>0.942149</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.473709</td>\n",
       "      <td>0.486843</td>\n",
       "      <td>0.080514</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.683604</td>\n",
       "      <td>0.993873</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.471810</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.042162</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>0.812510</td>\n",
       "      <td>0.976657</td>\n",
       "      <td>0.223313</td>\n",
       "      <td>0.363510</td>\n",
       "      <td>0.415684</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.223313</td>\n",
       "      <td>0.776687</td>\n",
       "      <td>0.998317</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.472184</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.027780</td>\n",
       "      <td>0.052258</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.802631</td>\n",
       "      <td>0.923313</td>\n",
       "      <td>0.192784</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.367547</td>\n",
       "      <td>0.050059</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.192784</td>\n",
       "      <td>0.807216</td>\n",
       "      <td>0.994951</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.493216</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.088155</td>\n",
       "      <td>0.049605</td>\n",
       "      <td>0.052525</td>\n",
       "      <td>0.015125</td>\n",
       "      <td>0.033825</td>\n",
       "      <td>0.839945</td>\n",
       "      <td>0.912560</td>\n",
       "      <td>0.367635</td>\n",
       "      <td>0.524121</td>\n",
       "      <td>0.515286</td>\n",
       "      <td>0.096586</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>0.367635</td>\n",
       "      <td>0.632365</td>\n",
       "      <td>0.988891</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>0.514874</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.091633</td>\n",
       "      <td>0.026325</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>0.839689</td>\n",
       "      <td>0.890735</td>\n",
       "      <td>0.377669</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.512923</td>\n",
       "      <td>0.101653</td>\n",
       "      <td>0.014610</td>\n",
       "      <td>0.377669</td>\n",
       "      <td>0.622331</td>\n",
       "      <td>0.985390</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.539128</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.108260</td>\n",
       "      <td>0.043261</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>0.054603</td>\n",
       "      <td>0.846343</td>\n",
       "      <td>0.875782</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.566310</td>\n",
       "      <td>0.535847</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.418446</td>\n",
       "      <td>0.581554</td>\n",
       "      <td>0.981283</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.016921</td>\n",
       "      <td>0.589778</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.120669</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.092362</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>0.061262</td>\n",
       "      <td>0.852024</td>\n",
       "      <td>0.861930</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.126785</td>\n",
       "      <td>0.023026</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>0.976974</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.021963</td>\n",
       "      <td>0.618610</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.141611</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.080007</td>\n",
       "      <td>0.854225</td>\n",
       "      <td>0.827156</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.619760</td>\n",
       "      <td>0.563461</td>\n",
       "      <td>0.143625</td>\n",
       "      <td>0.032653</td>\n",
       "      <td>0.495517</td>\n",
       "      <td>0.504483</td>\n",
       "      <td>0.967347</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.039348</td>\n",
       "      <td>0.635345</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.164985</td>\n",
       "      <td>0.051462</td>\n",
       "      <td>0.133908</td>\n",
       "      <td>0.056247</td>\n",
       "      <td>0.095078</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>0.799819</td>\n",
       "      <td>0.567250</td>\n",
       "      <td>0.663752</td>\n",
       "      <td>0.593780</td>\n",
       "      <td>0.170036</td>\n",
       "      <td>0.044772</td>\n",
       "      <td>0.567250</td>\n",
       "      <td>0.432750</td>\n",
       "      <td>0.955228</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.177417</td>\n",
       "      <td>0.018572</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.067336</td>\n",
       "      <td>0.104360</td>\n",
       "      <td>0.858320</td>\n",
       "      <td>0.763476</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.667308</td>\n",
       "      <td>0.586606</td>\n",
       "      <td>0.186108</td>\n",
       "      <td>0.057901</td>\n",
       "      <td>0.592656</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>0.942099</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.696462</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.178495</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.142298</td>\n",
       "      <td>0.068109</td>\n",
       "      <td>0.105203</td>\n",
       "      <td>0.858474</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>0.594791</td>\n",
       "      <td>0.668346</td>\n",
       "      <td>0.587374</td>\n",
       "      <td>0.186979</td>\n",
       "      <td>0.058372</td>\n",
       "      <td>0.594791</td>\n",
       "      <td>0.405209</td>\n",
       "      <td>0.941628</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.042791</td>\n",
       "      <td>0.702810</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.184179</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>0.138032</td>\n",
       "      <td>0.071359</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>0.864155</td>\n",
       "      <td>0.774324</td>\n",
       "      <td>0.611657</td>\n",
       "      <td>0.683445</td>\n",
       "      <td>0.605222</td>\n",
       "      <td>0.189384</td>\n",
       "      <td>0.056218</td>\n",
       "      <td>0.611657</td>\n",
       "      <td>0.388343</td>\n",
       "      <td>0.943782</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>0.704688</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.196430</td>\n",
       "      <td>0.067135</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>0.082776</td>\n",
       "      <td>0.114262</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>0.752932</td>\n",
       "      <td>0.630444</td>\n",
       "      <td>0.686265</td>\n",
       "      <td>0.602416</td>\n",
       "      <td>0.200747</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.630444</td>\n",
       "      <td>0.369556</td>\n",
       "      <td>0.934761</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>0.711351</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.203385</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.118353</td>\n",
       "      <td>0.086242</td>\n",
       "      <td>0.102298</td>\n",
       "      <td>0.865179</td>\n",
       "      <td>0.734661</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>0.709015</td>\n",
       "      <td>0.622060</td>\n",
       "      <td>0.223576</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>0.685098</td>\n",
       "      <td>0.314902</td>\n",
       "      <td>0.921969</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.764921</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.204770</td>\n",
       "      <td>0.056846</td>\n",
       "      <td>0.147274</td>\n",
       "      <td>0.086014</td>\n",
       "      <td>0.116644</td>\n",
       "      <td>0.866561</td>\n",
       "      <td>0.751149</td>\n",
       "      <td>0.663108</td>\n",
       "      <td>0.704388</td>\n",
       "      <td>0.620656</td>\n",
       "      <td>0.211650</td>\n",
       "      <td>0.069279</td>\n",
       "      <td>0.663108</td>\n",
       "      <td>0.336892</td>\n",
       "      <td>0.930721</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.202506</td>\n",
       "      <td>0.014980</td>\n",
       "      <td>0.139323</td>\n",
       "      <td>0.080465</td>\n",
       "      <td>0.109894</td>\n",
       "      <td>0.869939</td>\n",
       "      <td>0.752177</td>\n",
       "      <td>0.682323</td>\n",
       "      <td>0.715549</td>\n",
       "      <td>0.632765</td>\n",
       "      <td>0.217485</td>\n",
       "      <td>0.070895</td>\n",
       "      <td>0.682323</td>\n",
       "      <td>0.317677</td>\n",
       "      <td>0.929105</td>\n",
       "      <td>19537.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    demographic parity  predictive parity  equality opportunity   \n",
       "0             0.011385           0.993243              0.037206  \\\n",
       "1             0.024310           0.996835              0.079727   \n",
       "2             0.038655           0.006873              0.091722   \n",
       "3             0.021673           0.000000              0.017922   \n",
       "4             0.028683           0.005814              0.031614   \n",
       "5             0.036005           0.002134              0.043393   \n",
       "6             0.057346           0.006960              0.008146   \n",
       "7             0.054103           0.001775              0.010189   \n",
       "8             0.068367           0.026821              0.017659   \n",
       "9             0.042162           0.006250              0.010208   \n",
       "10            0.048800           0.027780              0.052258   \n",
       "11            0.088155           0.049605              0.052525   \n",
       "12            0.091633           0.026325              0.054717   \n",
       "13            0.108260           0.043261              0.085268   \n",
       "14            0.120669           0.056145              0.092362   \n",
       "15            0.141611           0.086868              0.115178   \n",
       "16            0.164985           0.051462              0.133908   \n",
       "17            0.177417           0.018572              0.141384   \n",
       "18            0.178495           0.020057              0.142298   \n",
       "19            0.184179           0.063885              0.138032   \n",
       "20            0.196430           0.067135              0.145749   \n",
       "21            0.203385           0.010185              0.118353   \n",
       "22            0.204770           0.056846              0.147274   \n",
       "23            0.202506           0.014980              0.139323   \n",
       "\n",
       "    predictive equality  average absolute odds  accuracy  precision    recall   \n",
       "0              0.000111               0.018658  0.767723   0.993243  0.031383  \\\n",
       "1              0.000111               0.039919  0.776322   0.996835  0.067250   \n",
       "2              0.000442               0.046082  0.791677   0.993569  0.131939   \n",
       "3              0.000000               0.008961  0.784153   1.000000  0.099701   \n",
       "4              0.000332               0.015973  0.790039   0.994898  0.124893   \n",
       "5              0.000491               0.021942  0.796796   0.990385  0.153928   \n",
       "6              0.003117               0.005632  0.825869   0.969941  0.282451   \n",
       "7              0.002454               0.006322  0.822337   0.972720  0.266439   \n",
       "8              0.007795               0.012727  0.831448   0.942149  0.316396   \n",
       "9              0.001349               0.005778  0.812510   0.976657  0.223313   \n",
       "10             0.006592               0.029425  0.802631   0.923313  0.192784   \n",
       "11             0.015125               0.033825  0.839945   0.912560  0.367635   \n",
       "12             0.017479               0.036098  0.839689   0.890735  0.377669   \n",
       "13             0.023938               0.054603  0.846343   0.875782  0.418446   \n",
       "14             0.030163               0.061262  0.852024   0.861930  0.455807   \n",
       "15             0.044837               0.080007  0.854225   0.827156  0.495517   \n",
       "16             0.056247               0.095078  0.862210   0.799819  0.567250   \n",
       "17             0.067336               0.104360  0.858320   0.763476  0.592656   \n",
       "18             0.068109               0.105203  0.858474   0.762661  0.594791   \n",
       "19             0.071359               0.104696  0.864155   0.774324  0.611657   \n",
       "20             0.082776               0.114262  0.861801   0.752932  0.630444   \n",
       "21             0.086242               0.102298  0.865179   0.734661  0.685098   \n",
       "22             0.086014               0.116644  0.866561   0.751149  0.663108   \n",
       "23             0.080465               0.109894  0.869939   0.752177  0.682323   \n",
       "\n",
       "    f1 score       mcc  selection rate  false positive rate   \n",
       "0   0.060844  0.154197        0.007575             0.000067  \\\n",
       "1   0.126000  0.227375        0.016174             0.000067   \n",
       "2   0.232944  0.320185        0.031837             0.000269   \n",
       "3   0.181324  0.278665        0.023903             0.000000   \n",
       "4   0.221927  0.311580        0.030097             0.000202   \n",
       "5   0.266445  0.345902        0.037263             0.000471   \n",
       "6   0.437500  0.468568        0.069816             0.002760   \n",
       "7   0.418301  0.455159        0.065670             0.002356   \n",
       "8   0.473709  0.486843        0.080514             0.006127   \n",
       "9   0.363510  0.415684        0.054819             0.001683   \n",
       "10  0.318969  0.367547        0.050059             0.005049   \n",
       "11  0.524121  0.515286        0.096586             0.011109   \n",
       "12  0.530435  0.512923        0.101653             0.014610   \n",
       "13  0.566310  0.535847        0.114552             0.018717   \n",
       "14  0.596285  0.555305        0.126785             0.023026   \n",
       "15  0.619760  0.563461        0.143625             0.032653   \n",
       "16  0.663752  0.593780        0.170036             0.044772   \n",
       "17  0.667308  0.586606        0.186108             0.057901   \n",
       "18  0.668346  0.587374        0.186979             0.058372   \n",
       "19  0.683445  0.605222        0.189384             0.056218   \n",
       "20  0.686265  0.602416        0.200747             0.065239   \n",
       "21  0.709015  0.622060        0.223576             0.078031   \n",
       "22  0.704388  0.620656        0.211650             0.069279   \n",
       "23  0.715549  0.632765        0.217485             0.070895   \n",
       "\n",
       "    true positive rate  false negative rate  true negative rate    count   \n",
       "0             0.031383             0.968617            0.999933  19537.0  \\\n",
       "1             0.067250             0.932750            0.999933  19537.0   \n",
       "2             0.131939             0.868061            0.999731  19537.0   \n",
       "3             0.099701             0.900299            1.000000  19537.0   \n",
       "4             0.124893             0.875107            0.999798  19537.0   \n",
       "5             0.153928             0.846072            0.999529  19537.0   \n",
       "6             0.282451             0.717549            0.997240  19537.0   \n",
       "7             0.266439             0.733561            0.997644  19537.0   \n",
       "8             0.316396             0.683604            0.993873  19537.0   \n",
       "9             0.223313             0.776687            0.998317  19537.0   \n",
       "10            0.192784             0.807216            0.994951  19537.0   \n",
       "11            0.367635             0.632365            0.988891  19537.0   \n",
       "12            0.377669             0.622331            0.985390  19537.0   \n",
       "13            0.418446             0.581554            0.981283  19537.0   \n",
       "14            0.455807             0.544193            0.976974  19537.0   \n",
       "15            0.495517             0.504483            0.967347  19537.0   \n",
       "16            0.567250             0.432750            0.955228  19537.0   \n",
       "17            0.592656             0.407344            0.942099  19537.0   \n",
       "18            0.594791             0.405209            0.941628  19537.0   \n",
       "19            0.611657             0.388343            0.943782  19537.0   \n",
       "20            0.630444             0.369556            0.934761  19537.0   \n",
       "21            0.685098             0.314902            0.921969  19537.0   \n",
       "22            0.663108             0.336892            0.930721  19537.0   \n",
       "23            0.682323             0.317677            0.929105  19537.0   \n",
       "\n",
       "    best_trial  fair_metric  model_metric model_name  \n",
       "0          2.0     0.000000      0.052761         RF  \n",
       "1          1.0     0.000111      0.129615         RF  \n",
       "2          3.0     0.000219      0.211406         RF  \n",
       "3         17.0     0.000328      0.298484       LGBM  \n",
       "4          4.0     0.000726      0.392168       LGBM  \n",
       "5          5.0     0.000812      0.398623       LGBM  \n",
       "6         19.0     0.000948      0.417735       LGBM  \n",
       "7         20.0     0.001751      0.427776       LGBM  \n",
       "8         18.0     0.003929      0.471810       LGBM  \n",
       "9          7.0     0.004248      0.472184       LGBM  \n",
       "10         6.0     0.006572      0.493216       LGBM  \n",
       "11        12.0     0.006675      0.514874       LGBM  \n",
       "12        14.0     0.011271      0.539128       LGBM  \n",
       "13        15.0     0.016921      0.589778       LGBM  \n",
       "14         9.0     0.021963      0.618610       LGBM  \n",
       "15         8.0     0.039348      0.635345       LGBM  \n",
       "16        13.0     0.039683      0.667200       LGBM  \n",
       "17        21.0     0.041186      0.696462       LGBM  \n",
       "18        22.0     0.042791      0.702810       LGBM  \n",
       "19        10.0     0.049580      0.704688       LGBM  \n",
       "20        11.0     0.052571      0.711351       LGBM  \n",
       "21        16.0     0.055012      0.764921       LGBM  \n",
       "22         NaN          NaN           NaN         RF  \n",
       "23         NaN          NaN           NaN       LGBM  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_trial = df_overall.loc[df_overall.index == 23,'best_trial'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(n_best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(df_overall.columns == 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metrics-f1-ppv-models-motpe-succesivehalving-parallel-150trials-1sim.pkl'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'f1-ppv-models-motpe-succesivehalving-parallel-150trials-4sim.pkl'\n",
    "\n",
    "'metrics-' + file_name[:-8] + str(sim_n) + 'sim.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.000000\n",
       "1     0.000111\n",
       "2     0.000219\n",
       "3     0.000328\n",
       "4     0.000726\n",
       "5     0.000812\n",
       "6     0.000948\n",
       "7     0.001751\n",
       "8     0.003929\n",
       "9     0.004248\n",
       "10    0.006572\n",
       "11    0.006675\n",
       "12    0.011271\n",
       "13    0.016921\n",
       "14    0.021963\n",
       "15    0.039348\n",
       "16    0.039683\n",
       "17    0.041186\n",
       "18    0.042791\n",
       "19    0.049580\n",
       "20    0.052571\n",
       "21    0.055012\n",
       "22         NaN\n",
       "23         NaN\n",
       "Name: fair_metric, dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overall.loc[:,'fair_metric']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
